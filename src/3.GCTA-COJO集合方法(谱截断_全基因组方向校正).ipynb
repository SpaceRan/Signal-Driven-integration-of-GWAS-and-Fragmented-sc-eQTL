{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370059e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.linalg import solve as solve_dense, pinv\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch \n",
    "from collections import  Counter\n",
    "import warnings\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## å‘Šè­¦è£…é¥°å™¨ ## \n",
    "def limit_warnings(max_count=10): \n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)   \n",
    "        def wrapper(*args, **kwargs):\n",
    "            if not hasattr(wrapper, '_warning_count'):\n",
    "                wrapper._warning_count = 0\n",
    "                wrapper._max_warnings = max_count\n",
    "            return func(*args, **kwargs)\n",
    "        \n",
    "        def controlled_warn(message, category=None):   # æä¾›ä¸€ä¸ªå®‰å…¨çš„ warn æ–¹æ³•\n",
    "            if wrapper._warning_count < wrapper._max_warnings:    \n",
    "                warnings.warn(message, category or UserWarning)\n",
    "                wrapper._warning_count += 1\n",
    "            else:\n",
    "                pass  # è¶…è¿‡æ¬¡æ•°ï¼Œé™é»˜å¿½ç•¥\n",
    "        \n",
    "        wrapper.warn = controlled_warn\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maximal_clique_blocks(R: np.ndarray, snp_ids: np.ndarray, r_min: float = 0.8) :\n",
    "    \"\"\"\n",
    "    å¯¹æ¯ä¸ªSNPï¼Œå®Œå…¨åŸºäº R â‰¥ r_min æ„å»º maximal cliques ä½œä¸ºå€™é€‰ block\n",
    "    Args:\n",
    "        R: LD çŸ©é˜µ (p x p), å·²æ’åºï¼Œå¯¹ç§°\n",
    "        snp_ids: SNP ID æ•°ç»„ (p,)ï¼Œå†…å®¹æ˜¯è¯¦ç»†çš„æ–‡æœ¬\n",
    "        r_min: æœ€å° R é˜ˆå€¼ï¼ˆæ­£ç›¸ä½ï¼‰ï¼Œç›®å‰è®¾ç½®ä¸º 0.8\n",
    "    Returns:\n",
    "        blocks: List of dict, dict åŒ…å«'snps'(ç´¢å¼•æ•°ç»„), 'snp_ids'(IDåˆ—è¡¨), 'size'\n",
    "    \"\"\"\n",
    "    p = R.shape[0]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(p))\n",
    "    \n",
    "    # ä¾æ¬¡æ·»åŠ æ­£ç›¸ä½å¼º LDï¼ŒåŒé‡å¾ªç¯ä¿è¯å…¨éƒ¨è¿é”\n",
    "    for i in range(p):\n",
    "        for j in range(i + 1, p):\n",
    "            if R[i, j] >= r_min:\n",
    "                G.add_edge(i, j)\n",
    "    \n",
    "    # æ‰¾æ‰€æœ‰ maximal cliquesï¼ŒnxåŒ…ä¿è¯åŠŸèƒ½å®ç°\n",
    "    cliques = list(nx.find_cliques(G)) ## å…³é”®æ˜¯è¿™é‡Œçš„è¾“å‡ºæ˜¯ä»€ä¹ˆ\n",
    "\n",
    "    blocks = []\n",
    "    for clique in cliques:\n",
    "        if len(clique) < 2:\n",
    "            continue\n",
    "        clique = sorted(clique)\n",
    "        \n",
    "        blocks.append({\n",
    "            'snps': np.array(clique),             ## è¿™é‡Œæ˜¯çº¯index\n",
    "            'snp_ids': snp_ids[clique].tolist(),  ## è¿™é‡Œæ˜¯æ ¹æ®indexæå–çš„è¯¦ç»†æ–‡æœ¬\n",
    "            'size': len(clique)\n",
    "        })\n",
    "    \n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_block_overlap(blocks, R):\n",
    "    \"\"\"\n",
    "    è§£å†³ block é—´ SNP é‡å ï¼Œä¼˜å…ˆä¿ç•™â€œå†…éƒ¨ LD å†…èšæ€§é«˜â€è€…ã€‚\n",
    "    ä¸¥æ ¼ä¿è¯è¾“å‡º block ä¹‹é—´æ—  SNP é‡å \n",
    "    æ³¨æ„ï¼Œè¿™é‡Œä¼ å…¥çš„RçŸ©é˜µå¯èƒ½æ˜¯numpyï¼Œæ³¨æ„['snps']æ˜¯indexä¸æ˜¯åå­—ï¼ˆåç»­æ£€æŸ¥ï¼‰\n",
    "    Args:\n",
    "        blocks: æ‰€æœ‰å€™é€‰ block åˆ—è¡¨ï¼ˆæ¥è‡ª find_maximal_clique_blocksï¼‰\n",
    "        R: LD çŸ©é˜µ (p, p)\n",
    "        æ’åºä¾æ®ï¼š'mean_r' ï¼Œä¸ä½¿ç”¨PC1ï¼Œå› ä¸ºå¯èƒ½åé¢è¿˜æœ‰å‰ªæï¼Œå¯¼è‡´PC1ä¸å¯é \n",
    "    Returns:\n",
    "        final_blocks: æ— é‡å çš„ block åˆ—è¡¨ï¼ˆdict æ ¼å¼åŒè¾“å…¥ï¼‰\n",
    "    \"\"\"\n",
    "    if not blocks:\n",
    "        return []\n",
    "    blocks_with_metric = []\n",
    "    for blk in blocks:\n",
    "        snps = blk['snps']\n",
    "        if len(snps) < 2:\n",
    "            continue\n",
    "        R_sub = R[np.ix_(snps, snps)]\n",
    "        upper_tri = R_sub[np.triu_indices_from(R_sub, k=1)]\n",
    "        mean_r = upper_tri.mean() if len(upper_tri) > 0 else 0.0\n",
    "        blk_with_metric = blk.copy()\n",
    "        blk_with_metric['mean_r'] = mean_r\n",
    "        blocks_with_metric.append(blk_with_metric)\n",
    "    if not blocks_with_metric:\n",
    "        return []\n",
    "    # æŒ‰ mean_r é™åºæ’åºï¼ˆä¼˜å…ˆä¿ç•™å†…èšæ€§å¼ºçš„ï¼‰\n",
    "    blocks_sorted = sorted(blocks_with_metric, key=lambda x: x['mean_r'], reverse=True)\n",
    "    # è´ªå¿ƒé€‰æ‹©ï¼šåªè¦æœ‰ä»»ä½• SNP é‡å ï¼Œå°±è·³è¿‡\n",
    "    final_blocks = []\n",
    "    used_snps = set()\n",
    "    for block in blocks_sorted:\n",
    "        member_set = set(block['snps'])\n",
    "        if used_snps & member_set:  # å­˜åœ¨é‡å  â†’ è·³è¿‡\n",
    "            continue\n",
    "        final_blocks.append(block)\n",
    "        used_snps |= member_set  # æ·»åŠ å½“å‰ block çš„æ‰€æœ‰ SNP\n",
    "\n",
    "    return final_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b07db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_prune_block(\n",
    "    block: dict,\n",
    "    R: np.ndarray,\n",
    "    pve_min: float = 0.7,\n",
    "    min_size: int = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    è¿­ä»£ä¿®å‰ªå•ä¸ª blockï¼Œç›´åˆ°å…¶ PC1 è§£é‡Šæ–¹å·®RçŸ©é˜µçš„æ¯”ä¾‹ â‰¥ pve_min\n",
    "    ZçŸ©é˜µ çš„æ–¹å‘æ˜¯ç»“æœï¼Œä¸æ˜¯å®šä¹‰ï¼Œå› æ­¤ä¸ä½œä¸ºä¿®å‰ªä¾æ®ï¼Œä¸å‚ä¸ä¿®å‰ª\n",
    "    å¹¶ä¸”å·²ç»ä¿è¯äº† block ä»£è¡¨ä¿¡å·ä¸ GWAS æ€»ä½“æ–¹å‘ä¸€è‡´ï¼Œå·²ç»å¤Ÿäº†\n",
    "    æˆåŠŸæ—¶è¿”å›åŒ…å« snpsã€loadingsã€pve ç­‰å­—æ®µçš„å¢å¼º block å­—å…¸ï¼›\n",
    "    å¤±è´¥æ—¶è¿”å› Noneã€‚\n",
    "    å…³é”®ä¿è¯ï¼š\n",
    "        - æ¯æ¬¡æ›´æ–°åä¼˜å…ˆæ£€æŸ¥ PVE\n",
    "        - ä¸»åŠ¨ä¿®å¤ç¡®ä¿åœ¨ç—…æ€ä¸‹ä»èƒ½æ¨è¿›\n",
    "        - è¿”å›ç»“æœå¯ç›´æ¥ç”¨äºåç»­ enrich å‡½æ•°\n",
    "    \"\"\"\n",
    "    curr = block['snps'].copy().tolist()\n",
    "    idx_to_id = dict(zip(block['snps'], block['snp_ids']))  # æå‰æ„å»º ID æ˜ å°„\n",
    "\n",
    "    while len(curr) >= min_size:\n",
    "        n = len(curr)\n",
    "        R_sub = R[np.ix_(curr, curr)]\n",
    "        # æ­¥éª¤1ï¼šå°è¯•åˆ†è§£ï¼Œæ£€æŸ¥æ˜¯å¦æ»¡è¶³ PVE\n",
    "        try:\n",
    "            eigvals = np.linalg.eigvalsh(R_sub)\n",
    "            lambda_max = eigvals[-1]\n",
    "            pve = lambda_max / n\n",
    "\n",
    "            if pve >= pve_min:\n",
    "                _, eigvecs = np.linalg.eigh(R_sub)\n",
    "                pc1 = eigvecs[:, -1]  \n",
    "\n",
    "                w = pc1 / np.linalg.norm(pc1)             ##  L2 å½’ä¸€åŒ–\n",
    "                snp_ids_kept = [idx_to_id[i] for i in curr]\n",
    "                return {\n",
    "                    'snps': np.array(curr),\n",
    "                    'snp_ids': snp_ids_kept,\n",
    "                    'loadings': w,           # å½’ä¸€åŒ–è½½è·ï¼Œä¹Ÿå°±æ˜¯PC1è§£é‡Šæ–¹å·®æ¯”ä¾‹\n",
    "                    'pve': float(pve),\n",
    "                    'size': len(curr),\n",
    "                    'mean_r': R_sub[np.triu_indices(n, k=1)].mean() if n > 1 else 0.0\n",
    "                }\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            pass  # è¿›å…¥ä¸»åŠ¨ä¿®å¤\n",
    "        \n",
    "        try:\n",
    "            _, eigvecs = np.linalg.eigh(R_sub)\n",
    "            pc1 = eigvecs[:, -1]\n",
    "            outlier_idx = np.argmin(np.abs(pc1))\n",
    "            curr.pop(outlier_idx)\n",
    "            continue\n",
    "        except np.linalg.LinAlgError:\n",
    "            pass\n",
    "\n",
    "        best_score = -1\n",
    "        best_idx = None\n",
    "        for i in range(len(curr)):\n",
    "            trial = curr[:i] + curr[i+1:]\n",
    "            if len(trial) < min_size:\n",
    "                continue\n",
    "            R_trial = R[np.ix_(trial, trial)]\n",
    "            try:\n",
    "                eigvals = np.linalg.eigvalsh(R_trial)\n",
    "                pve_candidate = eigvals[-1] / len(trial)\n",
    "                if pve_candidate > best_score:\n",
    "                    best_score = pve_candidate\n",
    "                    best_idx = i\n",
    "            except:\n",
    "                continue\n",
    "        if best_idx is not None:\n",
    "            curr.pop(best_idx)\n",
    "        else:\n",
    "            return None  # æ— æ³•ä¿®å¤\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47836090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_block_with_pca1_info(\n",
    "    block: dict,\n",
    "    R_full: np.ndarray,\n",
    "    z_gwas_full: np.ndarray,\n",
    "    z_qtl_full: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    ä¸ºå·²ä¿®å‰ªçš„ block æ·»åŠ å¯ç”¨äºåç»­å‰å‘é€‰æ‹©çš„ç»Ÿè®¡é‡ã€‚\n",
    "    å…³é”®è½¬æ¢ï¼šå°† evaluate_and_prune_block è¾“å‡ºçš„ L2-unitized loadings\n",
    "              è½¬æ¢ä¸ºæ»¡è¶³ Var(PC1) = 1 çš„å»ºæ¨¡å°ºåº¦è½½è·ã€‚\n",
    "    Args:\n",
    "        block: æ¥è‡ª evaluate_and_prune_block çš„è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«:\n",
    "               - 'snps': np.array of int, SNP ç´¢å¼•\n",
    "               - 'snp_ids': list of str, SNP ID åˆ—è¡¨\n",
    "               - 'loadings': np.array, PC1 è½½è·ï¼ˆL2 å½’ä¸€åŒ–ï¼Œå³ ||w||=1ï¼‰\n",
    "               - 'pve': float, PC1 è§£é‡Šæ–¹å·®æ¯”ä¾‹\n",
    "        R_full: (p, p) float, å…¨å±€ LD çŸ©é˜µï¼ˆå¯¹ç§°ã€æ ‡å‡†åŒ–ï¼‰\n",
    "        z_gwas_full: (p,) float, GWAS marginal Z åˆ†æ•°\n",
    "        z_qtl_full: (p,) float, QTL marginal Z åˆ†æ•°ï¼ˆå¯é€‰ç”¨é€”ï¼‰\n",
    "    Returns:\n",
    "        enhanced_block: dict, åŸå§‹ block çš„å¢å¼ºç‰ˆæœ¬ï¼Œæ–°å¢å­—æ®µï¼š\n",
    "            - 'loading_weights': np.array, æ»¡è¶³ w^T R_block w = 1 çš„è½½è·\n",
    "            - 'z_gwas_block': float, block ä»£è¡¨ Zï¼ˆåŸºäºæ ‡å‡†åŒ– PC1ï¼‰\n",
    "            - 'z_qtl_block': float, QTL æ–¹å‘ä¸Šçš„ block Z\n",
    "            - 'r_to_others': np.array (p,), block ä¸æ‰€æœ‰ SNP çš„åŠ æƒç›¸å…³æ€§\n",
    "    \"\"\"\n",
    "    snps_in_block = block['snps']\n",
    "    if len(snps_in_block) == 0:\n",
    "        raise ValueError(\"Block has no SNPs.\")\n",
    "    if R_full.shape[0] != R_full.shape[1]:\n",
    "        raise ValueError(\"R_full must be square.\")\n",
    "    p = R_full.shape[0]\n",
    "    if len(z_gwas_full) != p or len(z_qtl_full) != p:\n",
    "        raise ValueError(\"z_gwas_full and z_qtl_full must have length p.\")\n",
    "    # --- æå– block å†…éƒ¨ä¿¡æ¯ ---\n",
    "    members_idx = np.array(snps_in_block)\n",
    "    R_block = R_full[np.ix_(members_idx, members_idx)]  # (m, m)\n",
    "    w_l2 = np.array(block['loadings'])  # L2-unitized from pruning step\n",
    "\n",
    "    if len(w_l2) != len(members_idx):\n",
    "        raise ValueError(\"Length of 'loadings' does not match number of SNPs in block.\")\n",
    "    \n",
    "    # --- å…³é”®ï¼šå½’ä¸€åŒ–ä½¿å¾— Var(PC1) = 1ï¼Œå³ï¼šw^T R_block w = 1 ---\n",
    "    var_pc1 = w_l2 @ R_block @ w_l2\n",
    "    if var_pc1 < 1e-10:\n",
    "        raise ValueError(\n",
    "            f\"PC1 variance ({var_pc1:.2e}) too small. \"\n",
    "            \"Likely due to near-collinear SNPs or numerical instability.\"\n",
    "        )\n",
    "    scaling_factor = np.sqrt(var_pc1)\n",
    "    w_model = w_l2 / scaling_factor  # now w_model^T R_block w_model = 1\n",
    "    \n",
    "    var_pc1_normalized = w_model @ R_block @ w_model\n",
    "    assert np.isclose(var_pc1_normalized, 1.0, atol=1e-5), \\\n",
    "        f\"Normalization failed: Var(PC1) = {var_pc1_normalized:.3f} â‰  1.0\" ## \n",
    "\n",
    "    # --- æ ¡æ­£æ–¹å‘ï¼šç¡®ä¿ä¸ GWAS ä¿¡å·åŒå‘ ---\n",
    "    z_sub = z_gwas_full[members_idx]\n",
    "    if w_model @ z_sub < 0:\n",
    "        w_model = -w_model\n",
    "    # --- è®¡ç®— block-level Z åˆ†æ•° ---\n",
    "    z_gwas_block = float(w_model @ z_sub)\n",
    "    z_qtl_block = float(w_model @ z_qtl_full[members_idx])\n",
    "    # --- è®¡ç®— block ä¸æ‰€æœ‰ SNP çš„åŠ æƒ LDï¼ˆç›¸å…³æ€§å°ºåº¦ï¼‰---\n",
    "    # r_block,j = Î£_k w_k * r_k,j\n",
    "    r_to_others = w_model @ R_full[members_idx, :]  # (p,)\n",
    "    # --- æ„å»ºå¢å¼º block ---\n",
    "    enhanced_block = block.copy()\n",
    "    enhanced_block.update({\n",
    "        'loading_weights': w_model,           # ç”¨äºå»ºæ¨¡çš„æ ‡å‡†åŒ–è½½è·\n",
    "        'z_gwas_block': z_gwas_block,         # æ ‡å‡†åŒ–åçš„ block Z\n",
    "        'z_qtl_block': z_qtl_block,\n",
    "        'r_to_others': r_to_others            # é•¿åº¦ä¸º pï¼ŒåŒ…å«æ‰€æœ‰ SNPçš„ç›¸äº’R\n",
    "    })\n",
    "\n",
    "    return enhanced_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bed740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enriched_blocks_pipeline(\n",
    "    R: np.ndarray,\n",
    "    z_gwas: np.ndarray,\n",
    "    z_qtl: np.ndarray,\n",
    "    snp_ids: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    ä»æ— åˆ°æœ‰æ„å»º blockï¼Œå®Œæˆï¼šæ„å»º â†’ å»é‡ â†’ ä¿®å‰ª â†’ ä¿¡æ¯å¢å¼º\n",
    "    è¾“å‡ºï¼š\n",
    "        - enhanced blocks\n",
    "        - å‰©ä½™ SNP ç´¢å¼•\n",
    "        - block-block ç›¸å…³æ€§çŸ©é˜µ\n",
    "        - æ‰©å±•çš„ LD çŸ©é˜µ R_extended (p+B, p+B)ï¼Œæ”¯æŒ SNP + block ç»Ÿä¸€å»ºæ¨¡\n",
    "        - æ˜ å°„è¡¨ï¼šblock åœ¨æ‰©å±•çŸ©é˜µä¸­çš„ä½ç½®\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'blocks': list of enhanced_block,\n",
    "            'remaining_snp_idx': np.array,\n",
    "            'block_block_r_matrix': (B, B),\n",
    "            'R_extended': (p+B, p+B),\n",
    "            'block_positions_in_extended': list of int,  # é•¿åº¦ Bï¼Œè¡¨ç¤ºæ¯ä¸ª block åœ¨ R_extended ä¸­çš„åˆ—ç´¢å¼•\n",
    "        }\n",
    "    \"\"\"\n",
    "    p = R.shape[0]\n",
    "\n",
    "    assert len(z_gwas) == p and len(z_qtl) == p and len(snp_ids) == p, \"è¾“å…¥ç»´åº¦ä¸åŒ¹é…\"\n",
    "    if p == 0:\n",
    "        return {\n",
    "            'blocks': [],\n",
    "            'remaining_snp_idx': np.array([]),\n",
    "            'block_block_r_matrix': np.array([]).reshape(0, 0),\n",
    "            'R_extended': np.zeros((0, 0)),\n",
    "            'block_positions_in_extended': []\n",
    "        }\n",
    "\n",
    "    # Step 1: æ„å»º raw blocks\n",
    "    candidate_blocks = find_maximal_clique_blocks(R, snp_ids, r_min=0.8)\n",
    "    if not candidate_blocks:\n",
    "        return {\n",
    "            'blocks': [],\n",
    "            'remaining_snp_idx': np.arange(p),\n",
    "            'block_block_r_matrix': np.array([]).reshape(0, 0),\n",
    "            'R_extended': R.copy(),  # åŸå§‹ R\n",
    "            'block_positions_in_extended': []\n",
    "        }\n",
    "\n",
    "    # Step 2: å»é‡å‡½æ•°ï¼ˆä¸¥æ ¼æ— é‡å ï¼‰\n",
    "    valid_candidates = resolve_block_overlap(candidate_blocks, R)  \n",
    "\n",
    "    # Step 3: ä¿®å‰ªï¼ˆåŸºäº PVE â‰¥ 0.7ï¼‰\n",
    "    pruned_blocks = []\n",
    "    for blk in valid_candidates:\n",
    "        pruned = evaluate_and_prune_block(\n",
    "            block=blk,\n",
    "            R=R,\n",
    "            pve_min=0.7,\n",
    "            min_size=2\n",
    "        )\n",
    "        if pruned is not None:\n",
    "            pruned_blocks.append(pruned)\n",
    "\n",
    "    if not pruned_blocks:\n",
    "        return {\n",
    "            'blocks': [],\n",
    "            'remaining_snp_idx': np.arange(p),\n",
    "            'block_block_r_matrix': np.array([]).reshape(0, 0),\n",
    "            'R_extended': R.copy(),\n",
    "            'block_positions_in_extended': []\n",
    "        }\n",
    "\n",
    "    # Step 4: å¢å¼ºä¿¡æ¯ï¼ˆæ·»åŠ  z_block, r_to_others ç­‰ï¼‰\n",
    "    enriched_blocks = [\n",
    "        enrich_block_with_pca1_info(blk, R, z_gwas, z_qtl)\n",
    "        for blk in pruned_blocks\n",
    "    ]\n",
    "\n",
    "    # Step 5: è®¡ç®—æœªè¢« block è¦†ç›–çš„ SNP\n",
    "    used_snps = set()\n",
    "    for blk in enriched_blocks:\n",
    "        used_snps.update(blk['snps']) ## ä¸ºblockå†…éƒ¨çš„snp indexå†…å®¹\n",
    "        \n",
    "    remaining_snp_idx = np.array(sorted(set(range(p)) - used_snps)) ## ç”Ÿæˆæ‰€æœ‰æœªè¢«ä½¿ç”¨çš„SNP_index\n",
    "\n",
    "    # === Step 6: è®¡ç®— block-block ç›¸å…³æ€§çŸ©é˜µ ===\n",
    "    n_blocks = len(enriched_blocks)\n",
    "    block_block_r_matrix = np.eye(n_blocks)\n",
    "    for i in range(n_blocks):\n",
    "        blk_i = enriched_blocks[i]\n",
    "        w_i = blk_i['loading_weights']\n",
    "        idx_i = blk_i['snps']\n",
    "        for j in range(i + 1, n_blocks):\n",
    "            blk_j = enriched_blocks[j]\n",
    "            w_j = blk_j['loading_weights']\n",
    "            idx_j = blk_j['snps']\n",
    "            R_sub = R[np.ix_(idx_i, idx_j)]\n",
    "            r_ij = w_i @ R_sub @ w_j\n",
    "            block_block_r_matrix[i, j] = r_ij\n",
    "            block_block_r_matrix[j, i] = r_ij\n",
    "\n",
    "    # === Step 7: å…¨éƒ¨è®¡ç®—å®Œæˆï¼Œå¼€å§‹æ„å»ºæ‰©å±• R çŸ©é˜µ R_extended: (p + B) x (p + B) ===\n",
    "    B = n_blocks\n",
    "    R_extended = np.zeros((p + B, p + B))\n",
    "\n",
    "    # 1. åŸå§‹ SNP-SNP ç›¸å…³æ€§\n",
    "    R_extended[:p, :p] = R    ## åŸå§‹èµ‹å€¼\n",
    "\n",
    "    # 2. SNP - Block ç›¸å…³æ€§ï¼ˆåˆ©ç”¨æ¯ä¸ª block çš„ r_to_othersï¼‰\n",
    "    block_positions = []\n",
    "    for b_idx, block in enumerate(enriched_blocks):\n",
    "        pos_in_extended = p + b_idx                # block b_idx æ”¾åœ¨ç¬¬ p + b_idx åˆ—\n",
    "        block_positions.append(pos_in_extended)\n",
    "        r_to_others = block['r_to_others']         # (p,) å‘é‡ï¼Œblock PC1 ä¸æ‰€æœ‰ SNP çš„ç›¸å…³æ€§\n",
    "        \n",
    "        # å¡«å…¥ SNP - block è¡Œ/åˆ—\n",
    "        R_extended[:p, pos_in_extended] = r_to_others\n",
    "        R_extended[pos_in_extended, :p] = r_to_others  # å¯¹ç§°\n",
    "\n",
    "    # 3. Block - Block ç›¸å…³æ€§\n",
    "    R_extended[p:, p:] = block_block_r_matrix\n",
    "\n",
    "    # === è¿”å›ç»“æœ ===\n",
    "    return {\n",
    "        'blocks': enriched_blocks,\n",
    "        'remaining_snp_idx': remaining_snp_idx,\n",
    "        'block_block_r_matrix': block_block_r_matrix,\n",
    "        'R_extended': R_extended,\n",
    "        'block_positions_in_extended': block_positions,  # å¯æº¯æºï¼šç¬¬ i ä¸ª block åœ¨ R_extended ä¸­çš„ä½ç½®\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_sigma_ire(z_cond, tol=1e-2, max_iter=10, return_diagnostics=False):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨è¿­ä»£åŠ æƒä¸­ä½æ•°æ³•ä¼°è®¡æ®‹å·® Z çš„èƒŒæ™¯æ–¹å·® ÏƒÂ²\n",
    "    å‡è®¾å¤§å¤šæ•° SNP å±äºå™ªå£°ï¼ˆz_i ~ N(0, ÏƒÂ²)ï¼‰ï¼Œè€Œå°‘æ•°ä¸ºçœŸä¿¡å·\n",
    "    Args:\n",
    "        z_cond: (p,) conditional Z å‘é‡\n",
    "        tol: æ”¶æ•›é˜ˆå€¼\n",
    "        max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°\n",
    "        return_diagnostics: æ˜¯å¦è¿”å›è¯Šæ–­ä¿¡æ¯        \n",
    "    Returns:\n",
    "        sigma2: ä¼°è®¡çš„èƒŒæ™¯æ–¹å·®ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´å†… [0.8, 5.0]\n",
    "    \"\"\"\n",
    "    z = np.asarray(z_cond).flatten()\n",
    "    if len(z) == 0:\n",
    "        return (1.0, {}) if return_diagnostics else 1.0\n",
    "    ## åˆå§‹åŒ–\n",
    "    sigma2_initial =np.median(z**2) / 0.454936448\n",
    "    sigma2 = sigma2_initial\n",
    "    \n",
    "    for iter_idx in range(max_iter):\n",
    "        # é«˜æ–¯æ ¸æƒé‡ï¼š|z| è¶Šå¤§ï¼Œæƒé‡è¶Šå°\n",
    "        w = np.exp(-z**2 / (2 * sigma2 + 1e-8))\n",
    "        # åŠ æƒä¸­ä½æ•°è®¡ç®—\n",
    "        z2 = z**2\n",
    "        sorted_idx = np.argsort(z2)\n",
    "        z2_sorted = z2[sorted_idx]\n",
    "        w_sorted = w[sorted_idx]\n",
    "        cumw = np.cumsum(w_sorted)\n",
    "        total_weight = cumw[-1]\n",
    "        target = 0.5 * total_weight\n",
    "                \n",
    "        weighted_median_z2 = z2_sorted[np.searchsorted(cumw, target, side='right') - 1]\n",
    "        sigma2_new = weighted_median_z2 / 0.454936448\n",
    "        \n",
    "        sigma2_new = np.clip(sigma2_new, 0.8, 5.0)\n",
    "        \n",
    "        # æ£€æŸ¥æ”¶æ•›\n",
    "        if abs(sigma2_new - sigma2) < tol:\n",
    "            sigma2 = sigma2_new\n",
    "            break\n",
    "        sigma2 = sigma2_new\n",
    "\n",
    "    return sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@limit_warnings()\n",
    "def compute_conditional_z(\n",
    "    z: np.ndarray,\n",
    "    R: np.ndarray,\n",
    "    selected_indices: list,\n",
    "    U_trunc: np.ndarray,      # å…¨å±€è°±æˆªæ–­ç‰¹å¾å‘é‡\n",
    "    Lambda_trunc: np.ndarray  # å…¨å±€è°±æˆªæ–­ç‰¹å¾å€¼\n",
    "):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å…¨å±€è°±æˆªæ–­ç¨³å®šè®¡ç®— conditional Z (COJO æ€æƒ³)\n",
    "    \"\"\"\n",
    "    if len(selected_indices) == 0:\n",
    "        return z.copy()\n",
    "        \n",
    "    S = selected_indices\n",
    "    R_full_sub = R[:, S]      # (p, |S|) ä»æ‰€æœ‰ SNP åˆ°å·²é€‰ SNP çš„ LD\n",
    "    z_selected = z[S]         # (|S|,) å·²é€‰ SNP çš„ Z åˆ†æ•°\n",
    "    R_sub = R[np.ix_(S, S)]   # (|S|, |S|) å·²é€‰ SNP ä¹‹é—´çš„ LD\n",
    "\n",
    "    try:\n",
    "        U_global_S = U_trunc[S, :]  # (|S|, k) å·²é€‰ SNP åœ¨ä¸»æˆåˆ†ç©ºé—´ä¸­çš„è¡¨ç¤º\n",
    "        R_sub_inv = U_global_S @ np.diag(1.0 / Lambda_trunc) @ U_global_S.T\n",
    "        beta = R_sub_inv @ z_selected\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"compute_conditional_z è°±æˆªæ–­æ±‚é€†å¤±è´¥: {e}\")\n",
    "        try:\n",
    "            beta = np.linalg.solve(R_sub, z_selected)\n",
    "        except:\n",
    "            beta = np.linalg.pinv(R_sub) @ z_selected\n",
    "\n",
    "    # COJO æ ¸å¿ƒï¼šz_cond = z - R[:,S] @ Î²\n",
    "    proj_mean = R_full_sub @ beta\n",
    "    z_cond = z - proj_mean\n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d725d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spectral_truncation(R: np.ndarray, threshold: float = None):\n",
    "    \"\"\"\n",
    "    å¯¹ LD çŸ©é˜µè¿›è¡Œè°±æˆªæ–­ï¼Œè¿”å›æ˜¾è‘—ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼\n",
    "    \"\"\"\n",
    "    eigenvals, eigenvecs = np.linalg.eigh(R)    \n",
    "    idx = np.argsort(eigenvals)[::-1]\n",
    "    eigenvals = eigenvals[idx]\n",
    "    eigenvecs = eigenvecs[:, idx]\n",
    "    if threshold is None:\n",
    "        threshold = max(0.2, 1e-6 * eigenvals[0])  # ä¿®æ­£ï¼šä½¿ç”¨åˆç†çš„é»˜è®¤å€¼\n",
    "    \n",
    "    keep = eigenvals > threshold\n",
    "    if not np.any(keep):\n",
    "        keep[0] = True  # è‡³å°‘ä¿ç•™æœ€å¤§çš„ä¸€ä¸ª\n",
    "    U_trunc = eigenvecs[:, keep]\n",
    "    Lambda_trunc = eigenvals[keep]\n",
    "    print(f\"   apply_spectral_truncation è¯Šæ–­:\")\n",
    "    print(f\"     - åŸå§‹ç‰¹å¾å€¼èŒƒå›´: [{eigenvals.min():.6f}, {eigenvals.max():.6f}]\")\n",
    "    print(f\"     - æˆªæ–­é˜ˆå€¼: {threshold:.6f}\")\n",
    "    print(f\"     - ä¿ç•™ç‰¹å¾å€¼æ•°: {len(Lambda_trunc)}/{len(eigenvals)}\")\n",
    "    print(f\"     - Lambda_trunc èŒƒå›´: [{Lambda_trunc.min():.6f}, {Lambda_trunc.max():.6f}]\")\n",
    "    return U_trunc, Lambda_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcdd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@limit_warnings(max_count=30)\n",
    "def forward_selection(\n",
    "    z_raw: np.ndarray,\n",
    "    R: np.ndarray,\n",
    "    snp_ids: list,\n",
    "    U_trunc: np.ndarray,\n",
    "    sigma2: float,\n",
    "    Lambda_trunc: np.ndarray,\n",
    "    target_completion: float = 0.9):\n",
    "    \"\"\"\n",
    "    åŸºäº conditional Z å’Œ LD-adjusted ä¼ª RÂ² çš„å‰å‘é€‰æ‹©ç®—æ³•\n",
    "    \"\"\"\n",
    "    p = len(z_raw)\n",
    "    print(f\"ğŸ” forward_selection è¯Šæ–­:\")\n",
    "    print(f\"   - è¾“å…¥ç»´åº¦: p = {p}\")\n",
    "    print(f\"   - U_trunc å½¢çŠ¶: {U_trunc.shape}\")\n",
    "    print(f\"   - Lambda_trunc å½¢çŠ¶: {Lambda_trunc.shape}\")\n",
    "    print(f\"   - Lambda_trunc èŒƒå›´: [{Lambda_trunc.min():.6f}, {Lambda_trunc.max():.6f}]\")\n",
    "    \n",
    "    if snp_ids is None:\n",
    "        snp_ids = [f\"SNP_{i}\" for i in range(p)]\n",
    "\n",
    "    # === 1. èƒŒæ™¯æ–¹å·®æ ¡æ­£ ===\n",
    "    z = z_raw / np.sqrt(sigma2)\n",
    "    print(f\"   - èƒŒæ™¯æ–¹å·®ä¼°è®¡: {sigma2:.4f}\")\n",
    "    print(f\"   - æ ¡æ­£å Z èŒƒå›´: [{z.min():.4f}, {z.max():.4f}]\")\n",
    "    print(f\"   - æ ¡æ­£å Z å‡å€¼: {z.mean():.6f}\")\n",
    "    \n",
    "    # === 2. æ„é€ å…¨å±€èƒ½é‡ç®—å­ ===\n",
    "    P = U_trunc @ np.diag(1.0 / Lambda_trunc) @ U_trunc.T\n",
    "    print(f\"   - èƒ½é‡ç®—å­ P å½¢çŠ¶: {P.shape}\")\n",
    "    \n",
    "    # === 3. æ€»ä¿¡å·èƒ½é‡ ===\n",
    "    E_total = z @ P @ z\n",
    "    print(f\"   - æ€»ä¿¡å·èƒ½é‡ E_total: {E_total:.6f}\")\n",
    "\n",
    "    # === 4. åˆå§‹åŒ–çŠ¶æ€ ===\n",
    "    selected_indices = []\n",
    "    remaining_mask = np.ones(p, dtype=bool)\n",
    "    step = 0\n",
    "    completion = 0.0  # åˆå§‹åŒ– completion\n",
    "    print(f\"ğŸš€ å¼€å§‹å‰å‘é€‰æ‹©è¿­ä»£...\")\n",
    "    # === 5. ä¸»å¾ªç¯ ===\n",
    "    while True:\n",
    "        step += 1\n",
    "        if step > p + 10:  # é˜²æ­¢æ— é™å¾ªç¯\n",
    "            print(f\"âš ï¸  è¿­ä»£æ¬¡æ•°è¿‡å¤š ({step})ï¼Œå¼ºåˆ¶é€€å‡º\")\n",
    "            break\n",
    "        # --- 5.1 è®¡ç®—å½“å‰ conditional Z ---\n",
    "        try:\n",
    "            z_cond = compute_conditional_z(z, R, selected_indices, U_trunc, Lambda_trunc)\n",
    "            print(f\"   - z_cond è®¡ç®—æˆåŠŸï¼ŒèŒƒå›´: [{z_cond.min():.4f}, {z_cond.max():.4f}]\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ z_cond è®¡ç®—å¤±è´¥: {e}\")\n",
    "            break\n",
    "        try:\n",
    "            E_residual = z_cond @ P @ z_cond\n",
    "            E_explained = E_total - E_residual\n",
    "            completion = E_explained / E_total if E_total > 1e-8 else 1.0\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ èƒ½é‡è®¡ç®—å¤±è´¥: {e}\")\n",
    "            break\n",
    "\n",
    "        if completion >= target_completion:\n",
    "            print(f\"âœ… è¾¾åˆ°ç›®æ ‡å®Œæˆåº¦ {target_completion}ï¼Œåœæ­¢\")\n",
    "            break\n",
    "        if np.max(np.abs(z_cond[remaining_mask])) < 1.645:\n",
    "            print(f\"â¹ï¸  æœ€å¤§æ¡ä»¶ |z| < 1.645ï¼Œåœæ­¢\")\n",
    "            break\n",
    "        if len(selected_indices) == p:\n",
    "            print(f\"â¹ï¸  æ‰€æœ‰ SNP å·²é€‰æ‹©ï¼Œåœæ­¢\")\n",
    "            break\n",
    "\n",
    "        print(f\"   - å¯»æ‰¾å€™é€‰ SNP...\")\n",
    "        best_idx = None\n",
    "        best_completion = completion\n",
    "\n",
    "        candidate_count = 0\n",
    "        valid_candidates = 0\n",
    "        remaining_indices = np.where(remaining_mask)[0]\n",
    "        sorted_remaining = remaining_indices[np.argsort(np.abs(z_cond[remaining_indices]))[::-1]]\n",
    "        for idx in np.where(sorted_remaining)[0][:20]:  \n",
    "            candidate_count += 1\n",
    "            temp_selected = selected_indices + [idx]\n",
    "            try:\n",
    "                z_cond_temp = compute_conditional_z(z, R, temp_selected, U_trunc, Lambda_trunc)\n",
    "                E_residual_temp = z_cond_temp @ P @ z_cond_temp\n",
    "                E_explained_temp = E_total - E_residual_temp\n",
    "                completion_temp = E_explained_temp / E_total if E_total > 1e-8 else 1.0\n",
    "\n",
    "                if completion_temp > best_completion:\n",
    "                    best_completion = completion_temp\n",
    "                    best_idx = idx\n",
    "                    valid_candidates += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     å€™é€‰ SNP {idx} å¤±è´¥: {e}\")\n",
    "                continue\n",
    "        print(f\"   - å€™é€‰ SNP æ£€æŸ¥: {candidate_count} ä¸ª, æœ¬æ¬¡è¿­ä»£æœ‰æ•ˆ: {valid_candidates} ä¸ª\")\n",
    "        if best_idx is None:\n",
    "            print(\"âš ï¸  æ²¡æœ‰æ‰¾åˆ°èƒ½æå‡ completion çš„ SNP\")\n",
    "            break\n",
    "\n",
    "        selected_snp_id = snp_ids[best_idx]\n",
    "        print(f\"â¡ï¸  é€‰æ‹© SNP: {selected_snp_id} (index {best_idx})\")\n",
    "        selected_indices.append(best_idx)\n",
    "        remaining_mask[best_idx] = False\n",
    "\n",
    "    print(f\"\\nğŸ å‰å‘é€‰æ‹©å®Œæˆ:\")\n",
    "    print(f\"   - æ€»æ­¥æ•°: {step}\")\n",
    "    print(f\"   - æœ€ç»ˆé€‰ä¸­ SNP æ•°: {len(selected_indices)}\")\n",
    "    print(f\"   - æœ€ç»ˆ completion: {completion:.6f}\")\n",
    "    \n",
    "    # === 6. è¿”å›ç»“æœ ===\n",
    "    return {\n",
    "        'selected_indices': selected_indices,\n",
    "        'selected_snp_ids': [snp_ids[i] for i in selected_indices],\n",
    "        'n_selected': len(selected_indices),\n",
    "        'final_completion': float(completion)  # æ–°å¢è¾“å‡ºé¡¹\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_selection_paths(\n",
    "    blocks: list,           # list of dict, æ¯ä¸ª block åŒ…å« z_gwas_block, z_qtl_block, snps, ç­‰ç­‰\n",
    "    Z: np.ndarray,          # p, ä¸ºåŸå§‹ Z å‘é‡\n",
    "    R_extended: np.ndarray, # (p + B, p + B)ï¼Œå·²æ„å»ºå¥½çš„æ‰©å±• R\n",
    "    snp_list: list,         # (p,)ï¼ŒSNP ID åˆ—è¡¨\n",
    "    analysis_type: str,     # \"gwas\" or \"qtl\"\n",
    "    remaining_snp_idx: np.ndarray,  # è‡ªç”±æ±  SNP ç´¢å¼•\n",
    "    n_bootstraps: int = 100,\n",
    "    z_perturb_sd: float = 0.01\n",
    "):\n",
    "    '''\n",
    "    åŸºäº bootstrap çš„ç¨³å®š SNP/block é€‰æ‹©åˆ†æ\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n",
    "        - 'R_clean': ç”¨äºåˆ†æçš„ LD çŸ©é˜µ\n",
    "        - 'Z_clean': ç”¨äºåˆ†æçš„ Z åˆ†æ•°å‘é‡\n",
    "        - 'Z_extend': æ‰©å±•çš„ Z å‘é‡ï¼ˆåŒ…å« block ä¿¡æ¯ï¼‰\n",
    "        - 'sigma2': èƒŒæ™¯æ–¹å·®ä¼°è®¡\n",
    "        - 'all_selected_paths': æ¯æ¬¡ bootstrap çš„é€‰æ‹©è·¯å¾„\n",
    "        - 'selection_counter': é€‰æ‹©è®¡æ•°å™¨\n",
    "        - 'selection_frequency': é€‰æ‹©é¢‘ç‡\n",
    "        - 'stable_snp_id': é¢‘ç‡ > 90% çš„ç¨³å®š SNP/block ID\n",
    "        - 'snp_list_clean': æ¸…ç†åçš„ SNP ID åˆ—è¡¨\n",
    "        - 'clean_indices': æ¸…ç†åçš„ç´¢å¼•\n",
    "        - 'n_free_snps': è‡ªç”± SNP æ•°é‡\n",
    "        - 'n_blocks': block æ•°é‡\n",
    "        - 'avg_completion': å¹³å‡ä¿¡å·å®Œæˆåº¦  # æ–°å¢\n",
    "    '''\n",
    "    \n",
    "    # === Step 1: åŸºæœ¬éªŒè¯ ===\n",
    "    p = len(Z)\n",
    "    B = len(blocks)\n",
    "    assert len(snp_list) == p\n",
    "    assert R_extended.shape == (p + B, p + B)\n",
    "    # === Step 2: æ„å»ºæ‰©å±• Z å‘é‡ ===\n",
    "    Z_extended = np.zeros(p + B)\n",
    "    Z_extended[:p] = Z\n",
    "    SNPlist_extended = list(snp_list)\n",
    "\n",
    "    # æ·»åŠ  block ä¿¡æ¯\n",
    "    for i, block in enumerate(blocks):\n",
    "        pos = p + i\n",
    "        if analysis_type.lower() == 'gwas':\n",
    "            z_block = block['z_gwas_block']\n",
    "        elif analysis_type.lower() == 'qtl':\n",
    "            z_block = block['z_qtl_block']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown analysis_type: {analysis_type}\")\n",
    "        Z_extended[pos] = z_block\n",
    "        lead_snp = block['snp_ids'][0]\n",
    "        block_id = f\"block|{lead_snp}\"\n",
    "        SNPlist_extended.append(block_id)\n",
    "    # === Step 3: æ„å»º clean ç©ºé—´ ===\n",
    "    M = len(remaining_snp_idx)  # è‡ªç”± SNP æ•°é‡\n",
    "    N_clean = M + B\n",
    "    clean_indices = []\n",
    "    clean_indices.extend(remaining_snp_idx.tolist())\n",
    "    block_extended_positions = [p + i for i in range(B)]\n",
    "    clean_indices.extend(block_extended_positions)\n",
    "    assert len(clean_indices) == N_clean\n",
    "\n",
    "    # æå– clean R å’Œ Z\n",
    "    R_clean = R_extended[np.ix_(clean_indices, clean_indices)]\n",
    "    Z_clean = Z_extended[clean_indices]\n",
    "    snp_list_clean = [snp_list[idx] for idx in remaining_snp_idx] + [f\"block|{block['snp_ids'][0]}\" for block in blocks]\n",
    "\n",
    "    # === Step 4: é¢„å¤„ç†æ­¥éª¤ ===\n",
    "    estimate_sigma = estimate_sigma_ire(Z_clean)             # åœ¨ clean ç©ºé—´ä¸­ç›´æ¥ä¼°è®¡\n",
    "    U_trunc, Lambda_trunc = apply_spectral_truncation(R_clean)\n",
    "    \n",
    "    # === Step 5: Bootstrap åˆ†æ ===\n",
    "    print(f\"ğŸš€ å¼€å§‹ Bootstrap åˆ†æ:\")\n",
    "    print(f\"   - Clean ç©ºé—´å¤§å°: {len(clean_indices)} (è‡ªç”±SNP: {M}, Blocks: {B})\")\n",
    "    print(f\"   - èƒŒæ™¯æ–¹å·®ä¼°è®¡: {estimate_sigma:.4f}\")\n",
    "    print(f\"   - Bootstrap æ¬¡æ•°: {n_bootstraps}\")\n",
    "    all_selected = []\n",
    "    selection_counter = Counter()\n",
    "    successful_bootstraps = 0\n",
    "    completion_rates = []  # æ–°å¢ï¼šè®°å½•æ¯æ¬¡çš„å®Œæˆåº¦\n",
    "\n",
    "    for boot_idx in range(n_bootstraps):\n",
    "        if boot_idx % 50 == 0:\n",
    "            print(f\"   - Bootstrap è¿›åº¦: {boot_idx}/{n_bootstraps}\")\n",
    "        # Z åˆ†æ•°æ‰°åŠ¨ï¼Œä¼ å…¥çš„æ˜¯çº¯å‡€çš„Z-clean\n",
    "        Z_clean_perturbed = Z_clean + np.random.normal(0, z_perturb_sd, size=Z_clean.shape)\n",
    "        try:\n",
    "            result = forward_selection(\n",
    "                z_raw=Z_clean_perturbed,\n",
    "                R=R_clean,\n",
    "                snp_ids=snp_list_clean,\n",
    "                U_trunc=U_trunc,\n",
    "                sigma2= estimate_sigma , \n",
    "                Lambda_trunc=Lambda_trunc,\n",
    "                target_completion=0.9\n",
    "            )\n",
    "            selected_ids = result['selected_snp_ids']\n",
    "            if selected_ids:\n",
    "                successful_bootstraps += 1\n",
    "            all_selected.append(selected_ids)\n",
    "            selection_counter.update(selected_ids)\n",
    "            completion_rates.append(result['final_completion'])  # æ–°å¢ï¼šè®°å½•å®Œæˆåº¦\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Bootstrap {boot_idx} failed: {e}\")\n",
    "            completion_rates.append(0.0)  # å¤±è´¥çš„æƒ…å†µè®°å½•ä¸º0\n",
    "            continue\n",
    "\n",
    "    print(f\"   - æˆåŠŸçš„ Bootstrap æ¬¡æ•°: {successful_bootstraps}/{n_bootstraps}\")\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡å®Œæˆåº¦\n",
    "    avg_completion = np.mean(completion_rates) if completion_rates else 0.0\n",
    "    print(f\"   - å¹³å‡ä¿¡å·å®Œæˆåº¦: {avg_completion:.4f}\")\n",
    "    \n",
    "    selection_freq = {\n",
    "        snp_id: count / n_bootstraps\n",
    "        for snp_id, count in selection_counter.items()\n",
    "    }\n",
    "\n",
    "    for snp_id in snp_list_clean:\n",
    "        if snp_id not in selection_freq:\n",
    "            selection_freq[snp_id] = 0.0\n",
    "\n",
    "    stable_snp_id = [snp_id for snp_id, freq in selection_freq.items() if freq > 0.8]\n",
    "    return {\n",
    "        'R_clean': R_clean,\n",
    "        'Z_clean': Z_clean,\n",
    "        'Z_extend': Z_extended,\n",
    "        'sigma2': estimate_sigma,\n",
    "        'U_trunc': U_trunc,              \n",
    "        'Lambda_trunc': Lambda_trunc,       \n",
    "        'all_selected_paths': all_selected,\n",
    "        'selection_counter': selection_counter,\n",
    "        'selection_frequency': selection_freq,\n",
    "        'stable_snp_id': stable_snp_id,\n",
    "        'snp_list_clean': snp_list_clean,\n",
    "        'clean_indices': clean_indices,\n",
    "        'n_free_snps': M,\n",
    "        'n_blocks': B,\n",
    "        'avg_completion': float(avg_completion)  # æ–°å¢è¿”å›å€¼\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa42144",
   "metadata": {},
   "outputs": [],
   "source": [
    "@limit_warnings()\n",
    "def compute_stable_square_beta(\n",
    "    result: dict,\n",
    "    frequency_threshold: float = 0.9,\n",
    "    min_beta_weight: float = 1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    åŸºäº bootstrap ç»“æœï¼Œå¯¹é«˜é¢‘å…¥é€‰å˜é‡è¿›è¡Œå¤šå˜é‡æ•ˆåº”ä¼°è®¡\n",
    "    åœ¨ç»Ÿä¸€çš„è°±æˆªæ–­ç©ºé—´ä¸­è¿›è¡Œå›å½’ï¼Œä¿è¯ä¸ä¿¡å·å®Œæˆåº¦è®¡ç®—ä¸€è‡´æ€§\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    result : dict\n",
    "        bootstrap_selection_paths çš„è¿”å›ç»“æœï¼Œå¿…é¡»åŒ…å«ï¼š\n",
    "        - 'selection_frequency'\n",
    "        - 'snp_list_clean' \n",
    "        - 'R_clean'\n",
    "        - 'Z_clean'\n",
    "        - 'U_trunc', 'Lambda_trunc' (å…¨å±€è°±æˆªæ–­ç»“æœ)\n",
    "    frequency_threshold : float, default=0.9\n",
    "        å…¥é€‰é¢‘ç‡é˜ˆå€¼\n",
    "    min_beta_weight : float\n",
    "        é˜²æ­¢ betaÂ² å’Œä¸º 0 çš„ä¸‹ç•Œ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        å¢å¼ºçš„ç»“æœå­—å…¸\n",
    "    \"\"\"\n",
    "    # === Step 1: æå–å¿…è¦å­—æ®µ ===\n",
    "    selection_frequency = result['selection_frequency']\n",
    "    snp_list_clean = result['snp_list_clean']\n",
    "    R_clean = result['R_clean']\n",
    "    Z_clean_base = result['Z_clean']\n",
    "    U_trunc_global = result['U_trunc']            # å…¨å±€è°±æˆªæ–­ç‰¹å¾å‘é‡\n",
    "    Lambda_trunc_global = result['Lambda_trunc']  # å…¨å±€è°±æˆªæ–­ç‰¹å¾å€¼\n",
    "    \n",
    "    print(f\"ğŸ“ˆ compute_stable_square_beta è¾“å…¥è¯Šæ–­:\")\n",
    "    print(f\"   - selection_frequency ä¸­çš„ SNP æ•°: {len(selection_frequency)}\")\n",
    "    print(f\"   - snp_list_clean é•¿åº¦: {len(snp_list_clean)}\")\n",
    "    print(f\"   - R_clean å½¢çŠ¶: {R_clean.shape}\")\n",
    "    print(f\"   - å…¨å±€ä¸»æˆåˆ†æ•°é‡: {len(Lambda_trunc_global)}\")\n",
    "\n",
    "    # === Step 2: æ‰¾å‡ºé¢‘ç‡ > threshold çš„ç¨³å®šå˜é‡ ===\n",
    "    stable_snp_ids = [\n",
    "        snp_id for snp_id, freq in selection_frequency.items() \n",
    "        if freq >= frequency_threshold\n",
    "    ]\n",
    "\n",
    "    if len(stable_snp_ids) == 0:\n",
    "        print(f\"âš ï¸ No variable selected with frequency â‰¥ {frequency_threshold}. Skipping pseudobeta.\")\n",
    "        result.update({\n",
    "            'stable_snps': [],\n",
    "            'stable_snp_indices': [],\n",
    "            'beta_square': {},\n",
    "            'beta_multivar': {},\n",
    "            'beta_sorted': [],\n",
    "            'R_sub_condition_number': 0.0,\n",
    "        })\n",
    "        return result\n",
    "    \n",
    "    # === Step 3: æ˜ å°„ snp_id â†’ index in clean space ===\n",
    "    snp_to_idx = {snp: idx for idx, snp in enumerate(snp_list_clean)}\n",
    "    stable_indices = [snp_to_idx[snp_id] for snp_id in stable_snp_ids]\n",
    "    # === Step 4: æå–å­çŸ©é˜µ ===\n",
    "    R_sub = R_clean[np.ix_(stable_indices, stable_indices)]\n",
    "    z_sub = Z_clean_base[stable_indices]\n",
    "    k = len(stable_indices)\n",
    "    # === åœ¨ç»Ÿä¸€çš„è°±æˆªæ–­ç©ºé—´ä¸­æ±‚è§£ ===\n",
    "    # æå–å…¨å±€ç‰¹å¾å‘é‡åœ¨å­ç©ºé—´ä¸­çš„éƒ¨åˆ†\n",
    "    U_sub = U_trunc_global[stable_indices, :]  # shape: (k, n_components)\n",
    "    # æ„é€ æŠ•å½±åçš„å¹¿ä¹‰é€†ï¼šRâ»Â¹ â‰ˆ U_sub @ diag(1/Î›) @ U_sub.T\n",
    "    Lambda_inv = 1.0 / Lambda_trunc_global\n",
    "    R_sub_inv = U_sub @ np.diag(Lambda_inv) @ U_sub.T\n",
    "    # æ±‚è§£ beta = Râ»Â¹ z\n",
    "    beta = R_sub_inv @ z_sub\n",
    "    # æ¡ä»¶æ•°ä¼°è®¡ï¼ˆç”¨äºè¯Šæ–­ï¼‰\n",
    "    cond_num = np.linalg.cond(R_sub) if k > 1 else 1.0\n",
    "    \n",
    "    # === Step 6: è®¡ç®— beta å¹³æ–¹æƒé‡ ===\n",
    "    beta = np.asarray(beta).flatten()\n",
    "    weights = beta ** 2\n",
    "    total_weight = weights.sum()\n",
    "    # æ•°å€¼ç¨³å®šæ€§å¤„ç†\n",
    "    if total_weight < min_beta_weight or total_weight == 0:\n",
    "        beta_squared_weight = np.ones_like(weights) / len(weights) if len(weights) > 0 else np.array([])\n",
    "        compute_stable_square_beta.warn(\n",
    "            f\"Sum of beta^2 too small ({total_weight:.2e}). Using uniform weights.\"\n",
    "        )\n",
    "    else:\n",
    "        beta_squared_weight = weights / total_weight\n",
    "\n",
    "    # === Step 7: ç»‘å®šå› snp_id ===\n",
    "    beta_squared_dict = {\n",
    "        snp_id: float(beta2weight) \n",
    "        for snp_id, beta2weight in zip(stable_snp_ids, beta_squared_weight)\n",
    "    }\n",
    "    beta_dict = {\n",
    "        snp_id: float(b) \n",
    "        for snp_id, b in zip(stable_snp_ids, beta)\n",
    "    }\n",
    "    beta_sorted = sorted(beta_squared_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # === Step 8: æ›´æ–° result å¹¶è¿”å› ===\n",
    "    result.update({\n",
    "        'stable_snps': stable_snp_ids,\n",
    "        'stable_snp_indices': stable_indices,\n",
    "        'beta_square': beta_squared_dict,\n",
    "        'beta_multivar': beta_dict,\n",
    "        'beta_sorted': beta_sorted,\n",
    "        'R_sub_condition_number': float(cond_num),\n",
    "    })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_causal_discovery(\n",
    "    result: dict, \n",
    "    title: str = \"Causal Discovery by SNP Importance\", \n",
    "    figsize: tuple = (12, 6),\n",
    "    show_legend: bool = True,\n",
    "    bar_alpha: float = 0.8,\n",
    "    bar_colors: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–ç¨³å®šSNPçš„å› æœä¿¡å·è§£é‡Šèƒ½åŠ›ï¼ˆä»…æ˜¾ç¤ºbetaå¹³æ–¹æƒé‡ï¼‰\n",
    "    å¯è§†åŒ–å†…å®¹ï¼š\n",
    "    - æ¨ªè½´ï¼šstable SNP æŒ‰ beta_square é™åºæ’åˆ—\n",
    "    - çºµè½´ï¼šbeta_squareï¼ˆå¤šå˜é‡æ•ˆåº”å¹³æ–¹æƒé‡ï¼‰\n",
    "    - é¢œè‰²ï¼šblock ç”¨çº¢è‰²ï¼Œæ™®é€š SNP ç”¨å¤©è“è‰²    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    result : dict\n",
    "        compute_stable_square_beta çš„è¾“å‡ºï¼Œå¿…é¡»åŒ…å«ï¼š\n",
    "        - 'beta_square': dict, snp_id -> beta_square æƒé‡\n",
    "        - 'stable_snps': list of str, ç¨³å®š SNP ID åˆ—è¡¨\n",
    "        \n",
    "    title : str, default=\"Causal Discovery by SNP Importance\"\n",
    "        å›¾è¡¨æ ‡é¢˜\n",
    "        \n",
    "    figsize : tuple, default=(12, 6)\n",
    "        å›¾åƒå°ºå¯¸ (width, height)\n",
    "        \n",
    "    show_legend : bool, default=True\n",
    "        æ˜¯å¦æ˜¾ç¤ºå›¾ä¾‹\n",
    "        \n",
    "    bar_alpha : float, default=0.8\n",
    "        æŸ±çŠ¶å›¾é€æ˜åº¦\n",
    "        \n",
    "    bar_colors : dict, optional\n",
    "        è‡ªå®šä¹‰é¢œè‰²æ˜ å°„ï¼Œæ ¼å¼ï¼š{'block': color, 'snp': color}\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (fig, ax) matplotlib å›¾å½¢å¯¹è±¡\n",
    "    \"\"\"\n",
    "    # -----------------------------\n",
    "    # æå–æ•°æ®\n",
    "    # -----------------------------\n",
    "    beta_square_dict = result.get('beta_square', {})\n",
    "    stable_snp_ids = result.get('stable_snps', [])\n",
    "\n",
    "    # æ£€æŸ¥å¿…è¦æ•°æ®\n",
    "    if not stable_snp_ids or not beta_square_dict:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.text(0.5, 0.5, 'No stable SNPs/blocks identified', \n",
    "                transform=ax.transAxes, fontsize=14, color='gray', alpha=0.7, \n",
    "                ha='center', va='center')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        ax.set_ylabel(\"\")\n",
    "        plt.title(title, pad=20)\n",
    "        fig.tight_layout()\n",
    "        return fig, ax\n",
    "\n",
    "    # ----------------------------- \n",
    "    # æ ¸å¿ƒæ’åºï¼šæŒ‰ beta_square é™åº\n",
    "    # -----------------------------\n",
    "    sorted_by_beta2 = sorted(beta_square_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    snp_labels = [item[0] for item in sorted_by_beta2]\n",
    "    beta2_values = [item[1] for item in sorted_by_beta2]\n",
    "    x_pos = np.arange(len(snp_labels))\n",
    "    \n",
    "    # é¢œè‰²è®¾ç½®\n",
    "    if bar_colors is None:\n",
    "        bar_colors = {'block': 'red', 'snp': 'skyblue'}\n",
    "    \n",
    "    colors = [\n",
    "        bar_colors['block'] if snp.startswith('block|') else bar_colors['snp'] \n",
    "        for snp in snp_labels\n",
    "    ]\n",
    "\n",
    "    # -----------------------------\n",
    "    # ç»˜å›¾\n",
    "    # -----------------------------\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=300)\n",
    "    \n",
    "    # ç»˜åˆ¶ beta_square æŸ±çŠ¶å›¾\n",
    "    bars = ax.bar(x_pos, beta2_values, color=colors, alpha=bar_alpha, width=0.6)\n",
    "    ax.set_xlabel(\"Stable SNPs (ordered by $\\\\beta^2$ weight)\")\n",
    "    ax.set_ylabel(\"$\\\\beta^2$ Weight\", color='black')\n",
    "    ax.tick_params(axis='y', labelcolor='black')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(snp_labels, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_xlim(-0.6, len(snp_labels) - 0.4)\n",
    "\n",
    "    # å›¾ä¾‹\n",
    "    if show_legend:\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=bar_colors['snp'], label='SNP'),\n",
    "            Patch(facecolor=bar_colors['block'], label='Block')\n",
    "        ]\n",
    "        ax.legend(\n",
    "            handles=legend_elements,\n",
    "            loc='upper right',\n",
    "            frameon=True,\n",
    "            fontsize=9\n",
    "        )\n",
    "    \n",
    "    plt.title(title, pad=20)\n",
    "    fig.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fine_mapping_for_signal(\n",
    "        gene_df, ld_df, beta_col_gwas, se_col_gwas,\n",
    "        beta_col_qtl, se_col_qtl):\n",
    "    \"\"\"\n",
    "    å¯¹æŸä¸€ä¿¡å·è¿è¡Œå®Œæ•´æµç¨‹ï¼Œè¿”å› GWAS å’Œ QTL çš„åˆ†æç»“æœ,ä»¥åŠblockæ„é€ ä¿¡æ¯\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (result_raw_gwas, result_stable_gwas, \n",
    "            result_raw_qtl, result_stable_qtl)\n",
    "    \"\"\"\n",
    "    # === æ•°æ®å‡†å¤‡ ===\n",
    "    snps_list = ld_df.index.intersection(ld_df.columns).intersection(gene_df.index)\n",
    "    snps_list = snps_list.astype(str)\n",
    "    \n",
    "    ld_matrix_raw = ld_df.loc[snps_list, snps_list].values\n",
    "    \n",
    "    # Z åˆ†æ•°è®¡ç®—\n",
    "    beta_gwas = gene_df.loc[snps_list, beta_col_gwas].values\n",
    "    se_gwas = gene_df.loc[snps_list, se_col_gwas].values\n",
    "    z_gwas = beta_gwas / se_gwas \n",
    "\n",
    "    beta_qtl = gene_df.loc[snps_list, beta_col_qtl].values\n",
    "    se_qtl = gene_df.loc[snps_list, se_col_qtl].values\n",
    "    z_qtl = beta_qtl / se_qtl \n",
    "\n",
    "    # æ„å»º blocks\n",
    "    blocks_result = build_enriched_blocks_pipeline(ld_matrix_raw, z_gwas, z_qtl, snps_list)\n",
    "    r_extended = blocks_result[\"R_extended\"]\n",
    "    blocks = blocks_result['blocks']\n",
    "    remaining_snp = blocks_result['remaining_snp_idx']\n",
    "\n",
    "    print(f\"ğŸ“Š GWAS åˆ†æè¯Šæ–­:\")\n",
    "    print(f\"   - æ€» SNP æ•°: {len(snps_list)}\")\n",
    "    print(f\"   - Block æ•°: {len(blocks)}\")\n",
    "    print(f\"   - å‰©ä½™è‡ªç”± SNP: {len(remaining_snp)}\")\n",
    "    \n",
    "    # === GWAS åˆ†æ ===\n",
    "    result_raw_gwas = bootstrap_selection_paths(\n",
    "        blocks, z_gwas, r_extended, snps_list, \"gwas\", remaining_snp, 100, 0.01\n",
    "    )\n",
    "    \n",
    "    # GWAS è¯Šæ–­\n",
    "    print(f\"   - Bootstrap é€‰æ‹©è·¯å¾„æ•°: {len(result_raw_gwas.get('all_selected_paths', []))}\")\n",
    "    print(f\"   - ç¨³å®š SNP æ•°: {len(result_raw_gwas.get('stable_snp_id', []))}\")\n",
    "    if result_raw_gwas.get('stable_snp_id'):\n",
    "        print(f\"   - ç¨³å®š SNP ID: {result_raw_gwas['stable_snp_id'][:5]}...\")\n",
    "    \n",
    "    # GWAS ç¨³å®š SNP åˆ†æ\n",
    "    result_stable_gwas = compute_stable_square_beta(result_raw_gwas.copy(), frequency_threshold=0.9)\n",
    "    \n",
    "    # === QTL åˆ†æ ===\n",
    "    print(f\"ğŸ“Š QTL åˆ†æè¯Šæ–­:\")\n",
    "    print(f\"   - å‰©ä½™è‡ªç”± SNP: {len(remaining_snp)}\")\n",
    "    result_raw_qtl = bootstrap_selection_paths(\n",
    "        blocks, z_qtl, r_extended, snps_list, \"qtl\", remaining_snp, 100, 0.01\n",
    "    )\n",
    "    \n",
    "    # QTL è¯Šæ–­\n",
    "    print(f\"   - Bootstrap é€‰æ‹©è·¯å¾„æ•°: {len(result_raw_qtl.get('all_selected_paths', []))}\")\n",
    "    print(f\"   - ç¨³å®š SNP æ•°: {len(result_raw_qtl.get('stable_snp_id', []))}\")\n",
    "    if result_raw_qtl.get('stable_snp_id'):\n",
    "        print(f\"   - ç¨³å®š SNP ID: {result_raw_qtl['stable_snp_id'][:5]}...\")\n",
    "    \n",
    "    # QTL ç¨³å®š SNP åˆ†æ\n",
    "    result_stable_qtl = compute_stable_square_beta(result_raw_qtl.copy(), frequency_threshold=0.9)\n",
    "    \n",
    "    return (result_raw_gwas, result_stable_gwas,\n",
    "            result_raw_qtl, result_stable_qtl, blocks_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === æ–°å¢è¾…åŠ©å‡½æ•° ===\n",
    "def is_subset_np(a_arr, b_arr):\n",
    "    \"\"\"åˆ¤æ–­ä¸¤ä¸ªå­—ç¬¦ä¸²æ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ æ˜¯å¦æ»¡è¶³å­é›†å…³ç³»\"\"\"\n",
    "    result = []\n",
    "    for a, b in zip(a_arr, b_arr):\n",
    "        if pd.isna(a) or pd.isna(b):\n",
    "            result.append(False)\n",
    "            continue\n",
    "        str_a = str(a).upper()\n",
    "        str_b = str(b).upper()\n",
    "        set_a = set(str_a.split(','))\n",
    "        set_b = set(str_b.split(','))\n",
    "        if any(len(allele) > 1 and allele != '-' for allele in set_a | set_b):\n",
    "            result.append(False)\n",
    "            continue\n",
    "        result.append(set_a <= set_b or set_b <= set_a)\n",
    "    return np.array(result)\n",
    "\n",
    "def classify_and_adjust_beta_vectorized(df):\n",
    "    # æ ¹æ®å®é™…åˆ—åä¿®æ­£\n",
    "    required_cols = {'REF_GWAS', 'ALF_GWAS', 'REF_QTL', 'ALT_QTL', 'beta_QTL'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        missing = required_cols - set(df.columns)\n",
    "        raise ValueError(f\"ç¼ºå°‘å¿…éœ€åˆ—: {missing}\")\n",
    "    # ä½¿ç”¨å®é™…çš„åˆ—å\n",
    "    ref_qtl = df['REF_QTL'].values      # å¤§å†™\n",
    "    alt_qtl = df['ALT_QTL'].values      # å¤§å†™\n",
    "    ref_gwas = df['REF_GWAS'].values    # å¤§å†™\n",
    "    alt_gwas = df['ALF_GWAS'].values    # æ ¹æ®ä½ çš„åˆ—åæ˜¯ ALF_GWAS\n",
    "    beta_qtl = df['beta_QTL'].values    # å°å†™ä¿æŒä¸å˜\n",
    "\n",
    "    cond1 = is_subset_np(alt_gwas, alt_qtl) & is_subset_np(ref_gwas, ref_qtl)  # æ–¹å‘ä¸€è‡´\n",
    "    cond2 = is_subset_np(alt_gwas, ref_qtl) & is_subset_np(ref_gwas, alt_qtl)  # æ–¹å‘ç›¸å\n",
    "    valid_mask = cond1 | cond2\n",
    "    invalid_count = (~valid_mask).sum()\n",
    "    print(f\"å…± {invalid_count} è¡Œè¢«ä¸¢å¼ƒï¼ˆæ— æ³•å½’ç±»ï¼‰\")\n",
    "\n",
    "    adjusted_beta = np.where(cond2, -beta_qtl, beta_qtl)\n",
    "    df.loc[valid_mask, 'beta_QTL'] = adjusted_beta[valid_mask]\n",
    "\n",
    "    return df[valid_mask].copy()\n",
    "\n",
    "def pivot_ld_to_matrix(ld_df):\n",
    "    \"\"\"å°†ä¸‰åˆ—æ ¼å¼çš„ LD æ•°æ®è½¬æ¢ä¸ºå¯¹ç§°çŸ©é˜µ\"\"\"\n",
    "    ld_df = ld_df.drop_duplicates(subset=['ID_A', 'ID_B'])\n",
    "    all_snps = sorted(set(ld_df['ID_A']) | set(ld_df['ID_B']))\n",
    "    snp_map = {snp: i for i, snp in enumerate(all_snps)}\n",
    "    n = len(all_snps)\n",
    "    matrix = np.eye(n)  # é»˜è®¤å¯¹è§’çº¿ä¸º1\n",
    "\n",
    "    for _, row in ld_df.iterrows():\n",
    "        i = snp_map[row['ID_A']]\n",
    "        j = snp_map[row['ID_B']]\n",
    "        matrix[i, j] = matrix[j, i] = row['R']\n",
    "\n",
    "    return pd.DataFrame(matrix, index=all_snps, columns=all_snps)\n",
    "\n",
    "\n",
    "# === æ—¥å¿—è®°å½•å‡½æ•° ===\n",
    "\n",
    "def init_log_file(log_path):\n",
    "    \"\"\"åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶\"\"\"\n",
    "    if not log_path.exists():\n",
    "        pd.DataFrame(columns=[\n",
    "            'timestamp', 'csv_file', 'parquet_file', 'gene', 'status', \n",
    "            'common_snp_count', 'message'\n",
    "        ]).to_csv(log_path, index=False)\n",
    "\n",
    "def log_analysis(log_path, csv_file, parquet_file, gene, status, common_snp_count, message=\"\"):\n",
    "    \"\"\"è®°å½•åˆ†ææ—¥å¿—\"\"\"\n",
    "    log_entry = pd.DataFrame([{\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'csv_file': csv_file,\n",
    "        'parquet_file': parquet_file,\n",
    "        'gene': gene,\n",
    "        'status': status,\n",
    "        'common_snp_count': common_snp_count,\n",
    "        'message': message\n",
    "    }])\n",
    "    log_entry.to_csv(log_path, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c059ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(r\"D:\\desk\\study5_COPDxLC_SMR\\ç»“æœæ–‡ä»¶2\")\n",
    "log_file_path = folder_path / \"log_analysis.csv\"\n",
    "\n",
    "# åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶\n",
    "init_log_file(log_file_path)\n",
    "\n",
    "# è·å–æ‰€æœ‰CSVå’ŒParquetæ–‡ä»¶ï¼ˆä¸€å¯¹ä¸€æ˜ å°„ï¼‰\n",
    "csv_files = {f.stem: f for f in folder_path.glob(\"*.csv\")}\n",
    "parquet_files = {f.stem.replace('_LD_matrix', ''): f for f in folder_path.glob(\"*_LD_matrix.parquet\")}\n",
    "\n",
    "print(f\"ğŸ” æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶ï¼Œ{len(parquet_files)} ä¸ªParquetæ–‡ä»¶\")\n",
    "\n",
    "# éå†æ¯ä¸ªCSVæ–‡ä»¶\n",
    "for csv_prefix, csv_path in csv_files.items():\n",
    "    print(f\"\\nğŸ‘‰ æ­£åœ¨å¤„ç†: {csv_prefix}\")\n",
    "    \n",
    "    # æŸ¥æ‰¾å¯¹åº”çš„Parquetæ–‡ä»¶\n",
    "    pq_path = parquet_files.get(csv_prefix)\n",
    "    pq_prefix = pq_path.stem if pq_path else \"\"\n",
    "\n",
    "    # è¯»å–CSVæ–‡ä»¶\n",
    "    # åœ¨æ•°æ®è¯»å–åæ·»åŠ å»é‡é€»è¾‘ï¼ŒåŸºäºP_GWASå€¼\n",
    "    df_full = pd.read_csv(csv_path)\n",
    "    original_rows = len(df_full)\n",
    "    \n",
    "    # å¦‚æœæœ‰P_GWASåˆ—ï¼ŒåŸºäºP_GWASå»é‡ï¼Œä¿ç•™På€¼æœ€å°çš„\n",
    "    if 'P_GWAS' in df_full.columns:\n",
    "        # æŒ‰SNPåˆ†ç»„ï¼Œé€‰æ‹©P_GWASæœ€å°çš„è¡Œ\n",
    "        df_full = df_full.loc[df_full.groupby('SNP')['P_GWAS'].idxmin()]\n",
    "        if len(df_full) < original_rows:\n",
    "            print(f\"ğŸ§¹ åŸºäºP_GWASå»é‡: {original_rows} â†’ {len(df_full)} è¡Œ (ä¿ç•™På€¼æœ€å°)\")\n",
    "    \n",
    "    csv_snp_count = len(df_full['SNP'].unique())\n",
    "\n",
    "    try:\n",
    "        ld_long = pd.read_parquet(pq_path, engine='fastparquet')\n",
    "        pq_snp_set = set(ld_long['ID_A']) | set(ld_long['ID_B'])\n",
    "        pq_snp_count = len(pq_snp_set)\n",
    "        \n",
    "        coverage_ratio = (pq_snp_count / csv_snp_count) * 100 if csv_snp_count > 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–Parquetå¤±è´¥: {e}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", 0, f\"è¯»å–Parquetå¤±è´¥: {e}\")\n",
    "        continue\n",
    "    \n",
    "    output_pkl = folder_path / f\"{pq_prefix}.pkl\"\n",
    "    output_gwas_png = folder_path / f\"{pq_prefix}_gwas.png\"\n",
    "    output_qtl_png = folder_path / f\"{pq_prefix}_qtl.png\"\n",
    "    output_blank = folder_path / f\"{pq_prefix}_blank\"\n",
    "    \n",
    "    # æ£€æŸ¥_blankæ–‡ä»¶\n",
    "    if output_blank.exists():\n",
    "        print(f\"â­ï¸  å·²çŸ¥é—®é¢˜ï¼ˆSNPä¸è¶³ï¼‰ï¼Œè·³è¿‡\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"SKIPPED\", 0, \n",
    "                    f\"å·²çŸ¥é—®é¢˜ï¼šSNPä¸è¶³ | CSV SNPæ•°: {csv_snp_count}, Parquet SNPæ•°: {pq_snp_count}, è¦†ç›–ç‡: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ\n",
    "    if output_pkl.exists() and output_gwas_png.exists() and output_qtl_png.exists():\n",
    "        print(f\"âœ… å·²å®Œæˆï¼Œè·³è¿‡\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"SKIPPED\", 0, \n",
    "                    f\"å·²å®Œæˆï¼Œè·³è¿‡ | CSV SNPæ•°: {csv_snp_count}, Parquet SNPæ•°: {pq_snp_count}, è¦†ç›–ç‡: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    # è°ƒæ•´ beta_QTL æ–¹å‘ï¼ˆå¯¹å®Œæ•´æ•°æ®è¿›è¡Œå¤„ç†ï¼‰\n",
    "    try:\n",
    "        df_adjusted = classify_and_adjust_beta_vectorized(df_full)\n",
    "        print(f\"ğŸ“Š æ•°æ®è¿‡æ»¤ï¼šåŸå§‹ {len(df_full)} è¡Œ â†’ è¿‡æ»¤å {len(df_adjusted)} è¡Œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è°ƒæ•´ beta å¤±è´¥: {e}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", 0, \n",
    "                    f\"è°ƒæ•´betaå¤±è´¥: {e} | CSV SNPæ•°: {csv_snp_count}, Parquet SNPæ•°: {pq_snp_count}, è¦†ç›–ç‡: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "\n",
    "    # è®¾ç½®ç´¢å¼•\n",
    "    df_adjusted = df_adjusted.set_index('SNP')\n",
    "    ## è¿™ä¸ªæ˜¯è°ƒæ•´åçš„ï¼Œç„¶åå»LDé‡Œé¢æ‰¾ç›¸å…³å†…å®¹\n",
    "    try:\n",
    "        ld_df = pivot_ld_to_matrix(ld_long)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"è½¬æ¢Parquetå¤±è´¥: {e}\"\n",
    "        print(f\"âŒ {error_msg}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", 0, \n",
    "                    f\"{error_msg} | CSV SNPæ•°: {csv_snp_count}, Parquet SNPæ•°: {pq_snp_count}, è¦†ç›–ç‡: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "\n",
    "    # ç­›é€‰å…±åŒSNPï¼ˆä½¿ç”¨è°ƒæ•´åçš„æ•°æ®ç´¢å¼•ï¼‰ï¼Œåæ­£æ˜¯LDå†…å®¹æ˜¯è·Ÿç€df_adjustedèµ°çš„ï¼Œdf_adjustedçš„æ„æ€æ˜¯è°ƒæ•´æ–¹å‘åçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬\n",
    "    snps_common = ld_df.index.intersection(ld_df.columns).intersection(df_adjusted.index)\n",
    "    common_snp_count = len(snps_common)\n",
    "    ## è¿™é‡Œçš„æ„æ€æ˜¯ï¼Œå¦‚æœLDé‡Œé¢æ²¡æ‰¾åˆ°ã€‚ã€‚ã€‚é‚£ä¹ˆä¹Ÿåˆ å»\n",
    "    if common_snp_count < 3:\n",
    "        print(f\"âš ï¸ å…±åŒSNPæ•°é‡ä¸è¶³ï¼ˆ{common_snp_count} < 3ï¼‰ï¼Œè·³è¿‡åˆ†æ\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"SKIPPED\", common_snp_count, \n",
    "                    f\"å…±åŒSNPæ•°é‡ä¸è¶³ | CSV SNPæ•°: {csv_snp_count}, Parquet SNPæ•°: {pq_snp_count}, è¦†ç›–ç‡: {coverage_ratio:.2f}%\")\n",
    "        output_blank.touch()\n",
    "        continue\n",
    "    \n",
    "    # æå–å…±åŒSNPçš„å­é›†\n",
    "    ld_df = ld_df.loc[snps_common, snps_common]\n",
    "    df_sub = df_adjusted.loc[snps_common]        \n",
    "    # åˆ°è¿™é‡Œå®Œå…¨è°ƒæ•´å®Œæˆ\n",
    "    \n",
    "    # å¼€å§‹åˆ†æï¼Œè¾“å…¥çš„æ˜¯æ¸…ç†åçš„LDä»¥åŠ df_sub \n",
    "    try:\n",
    "        (result_raw_gwas, result_stable_gwas,\n",
    "         result_raw_qtl, result_stable_qtl, blocks_result) = run_fine_mapping_for_signal(\n",
    "            df_sub, ld_df, 'BETA_GWAS', 'SE_GWAS', 'beta_QTL', 'SE_QTL'\n",
    "        )\n",
    "        print(f\"âœ… åˆ†æå®Œæˆï¼Œå…±åŒSNPæ•°: {common_snp_count}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"SUCCESS\", common_snp_count, \n",
    "                    f\"åˆ†æå®Œæˆ | CSV SNPæ•°: {csv_snp_count}, Parquet SNPæ•°: {pq_snp_count}, è¦†ç›–ç‡: {coverage_ratio:.2f}%\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"åˆ†æå¤±è´¥: {e}\"\n",
    "        print(f\"âŒ {error_msg}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", common_snp_count, \n",
    "                    f\"{error_msg} | CSV SNPæ•°: {csv_snp_count}, Parquet SNPæ•°: {pq_snp_count}, è¦†ç›–ç‡: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    # ç»˜å›¾\n",
    "    try:\n",
    "        fig_gwas, _ = plot_causal_discovery(result_stable_gwas, f\"{csv_prefix}_gwas\")\n",
    "        fig_qtl, _ = plot_causal_discovery(result_stable_qtl, f\"{csv_prefix}_qtl\")\n",
    "        fig_gwas.savefig(output_gwas_png, dpi=300, bbox_inches='tight')\n",
    "        fig_qtl.savefig(output_qtl_png, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig_gwas)\n",
    "        plt.close(fig_qtl)\n",
    "        print(f\"âœ… ç»˜å›¾å®Œæˆ\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"ç»˜å›¾å¤±è´¥: {e}\"\n",
    "        print(f\"âŒ {error_msg}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", common_snp_count, \n",
    "                    f\"{error_msg} | CSV SNPæ•°: {csv_snp_count}, Parquet SNPæ•°: {pq_snp_count}, è¦†ç›–ç‡: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    try:\n",
    "        combined = {\n",
    "            \"gwas_bootstrap\": result_raw_gwas,\n",
    "            \"gwas_stable\": result_stable_gwas,\n",
    "            \"qtl_bootstrap\": result_raw_qtl,\n",
    "            \"qtl_stable\": result_stable_qtl,\n",
    "            \"block\": blocks_result\n",
    "        }\n",
    "        with open(output_pkl, 'wb') as f:\n",
    "            pickle.dump(combined, f)\n",
    "        print(f\"âœ… ç»“æœå·²ä¿å­˜è‡³ {output_pkl}\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"ä¿å­˜å¤±è´¥: {e}\"\n",
    "        print(f\"âŒ {error_msg}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", common_snp_count, \n",
    "                    f\"{error_msg} | CSV SNPæ•°: {csv_snp_count}, Parquet SNPæ•°: {pq_snp_count}, è¦†ç›–ç‡: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "\n",
    "print(\"âœ… å…¨éƒ¨ä»»åŠ¡å®Œæˆ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
