{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370059e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.linalg import solve as solve_dense, pinv\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch \n",
    "from collections import  Counter\n",
    "import warnings\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 告警装饰器 ## \n",
    "def limit_warnings(max_count=10): \n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)   \n",
    "        def wrapper(*args, **kwargs):\n",
    "            if not hasattr(wrapper, '_warning_count'):\n",
    "                wrapper._warning_count = 0\n",
    "                wrapper._max_warnings = max_count\n",
    "            return func(*args, **kwargs)\n",
    "        \n",
    "        def controlled_warn(message, category=None):   # 提供一个安全的 warn 方法\n",
    "            if wrapper._warning_count < wrapper._max_warnings:    \n",
    "                warnings.warn(message, category or UserWarning)\n",
    "                wrapper._warning_count += 1\n",
    "            else:\n",
    "                pass  # 超过次数，静默忽略\n",
    "        \n",
    "        wrapper.warn = controlled_warn\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maximal_clique_blocks(R: np.ndarray, snp_ids: np.ndarray, r_min: float = 0.8) :\n",
    "    \"\"\"\n",
    "    对每个SNP，完全基于 R ≥ r_min 构建 maximal cliques 作为候选 block\n",
    "    Args:\n",
    "        R: LD 矩阵 (p x p), 已排序，对称\n",
    "        snp_ids: SNP ID 数组 (p,)，内容是详细的文本\n",
    "        r_min: 最小 R 阈值（正相位），目前设置为 0.8\n",
    "    Returns:\n",
    "        blocks: List of dict, dict 包含'snps'(索引数组), 'snp_ids'(ID列表), 'size'\n",
    "    \"\"\"\n",
    "    p = R.shape[0]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(p))\n",
    "    \n",
    "    # 依次添加正相位强 LD，双重循环保证全部连锁\n",
    "    for i in range(p):\n",
    "        for j in range(i + 1, p):\n",
    "            if R[i, j] >= r_min:\n",
    "                G.add_edge(i, j)\n",
    "    \n",
    "    # 找所有 maximal cliques，nx包保证功能实现\n",
    "    cliques = list(nx.find_cliques(G)) ## 关键是这里的输出是什么\n",
    "\n",
    "    blocks = []\n",
    "    for clique in cliques:\n",
    "        if len(clique) < 2:\n",
    "            continue\n",
    "        clique = sorted(clique)\n",
    "        \n",
    "        blocks.append({\n",
    "            'snps': np.array(clique),             ## 这里是纯index\n",
    "            'snp_ids': snp_ids[clique].tolist(),  ## 这里是根据index提取的详细文本\n",
    "            'size': len(clique)\n",
    "        })\n",
    "    \n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_block_overlap(blocks, R):\n",
    "    \"\"\"\n",
    "    解决 block 间 SNP 重叠，优先保留“内部 LD 内聚性高”者。\n",
    "    严格保证输出 block 之间无 SNP 重叠\n",
    "    注意，这里传入的R矩阵可能是numpy，注意['snps']是index不是名字（后续检查）\n",
    "    Args:\n",
    "        blocks: 所有候选 block 列表（来自 find_maximal_clique_blocks）\n",
    "        R: LD 矩阵 (p, p)\n",
    "        排序依据：'mean_r' ，不使用PC1，因为可能后面还有剪枝，导致PC1不可靠\n",
    "    Returns:\n",
    "        final_blocks: 无重叠的 block 列表（dict 格式同输入）\n",
    "    \"\"\"\n",
    "    if not blocks:\n",
    "        return []\n",
    "    blocks_with_metric = []\n",
    "    for blk in blocks:\n",
    "        snps = blk['snps']\n",
    "        if len(snps) < 2:\n",
    "            continue\n",
    "        R_sub = R[np.ix_(snps, snps)]\n",
    "        upper_tri = R_sub[np.triu_indices_from(R_sub, k=1)]\n",
    "        mean_r = upper_tri.mean() if len(upper_tri) > 0 else 0.0\n",
    "        blk_with_metric = blk.copy()\n",
    "        blk_with_metric['mean_r'] = mean_r\n",
    "        blocks_with_metric.append(blk_with_metric)\n",
    "    if not blocks_with_metric:\n",
    "        return []\n",
    "    # 按 mean_r 降序排序（优先保留内聚性强的）\n",
    "    blocks_sorted = sorted(blocks_with_metric, key=lambda x: x['mean_r'], reverse=True)\n",
    "    # 贪心选择：只要有任何 SNP 重叠，就跳过\n",
    "    final_blocks = []\n",
    "    used_snps = set()\n",
    "    for block in blocks_sorted:\n",
    "        member_set = set(block['snps'])\n",
    "        if used_snps & member_set:  # 存在重叠 → 跳过\n",
    "            continue\n",
    "        final_blocks.append(block)\n",
    "        used_snps |= member_set  # 添加当前 block 的所有 SNP\n",
    "\n",
    "    return final_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b07db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_prune_block(\n",
    "    block: dict,\n",
    "    R: np.ndarray,\n",
    "    pve_min: float = 0.7,\n",
    "    min_size: int = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    迭代修剪单个 block，直到其 PC1 解释方差R矩阵的比例 ≥ pve_min\n",
    "    Z矩阵 的方向是结果，不是定义，因此不作为修剪依据，不参与修剪\n",
    "    并且已经保证了 block 代表信号与 GWAS 总体方向一致，已经够了\n",
    "    成功时返回包含 snps、loadings、pve 等字段的增强 block 字典；\n",
    "    失败时返回 None。\n",
    "    关键保证：\n",
    "        - 每次更新后优先检查 PVE\n",
    "        - 主动修复确保在病态下仍能推进\n",
    "        - 返回结果可直接用于后续 enrich 函数\n",
    "    \"\"\"\n",
    "    curr = block['snps'].copy().tolist()\n",
    "    idx_to_id = dict(zip(block['snps'], block['snp_ids']))  # 提前构建 ID 映射\n",
    "\n",
    "    while len(curr) >= min_size:\n",
    "        n = len(curr)\n",
    "        R_sub = R[np.ix_(curr, curr)]\n",
    "        # 步骤1：尝试分解，检查是否满足 PVE\n",
    "        try:\n",
    "            eigvals = np.linalg.eigvalsh(R_sub)\n",
    "            lambda_max = eigvals[-1]\n",
    "            pve = lambda_max / n\n",
    "\n",
    "            if pve >= pve_min:\n",
    "                _, eigvecs = np.linalg.eigh(R_sub)\n",
    "                pc1 = eigvecs[:, -1]  \n",
    "\n",
    "                w = pc1 / np.linalg.norm(pc1)             ##  L2 归一化\n",
    "                snp_ids_kept = [idx_to_id[i] for i in curr]\n",
    "                return {\n",
    "                    'snps': np.array(curr),\n",
    "                    'snp_ids': snp_ids_kept,\n",
    "                    'loadings': w,           # 归一化载荷，也就是PC1解释方差比例\n",
    "                    'pve': float(pve),\n",
    "                    'size': len(curr),\n",
    "                    'mean_r': R_sub[np.triu_indices(n, k=1)].mean() if n > 1 else 0.0\n",
    "                }\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            pass  # 进入主动修复\n",
    "        \n",
    "        try:\n",
    "            _, eigvecs = np.linalg.eigh(R_sub)\n",
    "            pc1 = eigvecs[:, -1]\n",
    "            outlier_idx = np.argmin(np.abs(pc1))\n",
    "            curr.pop(outlier_idx)\n",
    "            continue\n",
    "        except np.linalg.LinAlgError:\n",
    "            pass\n",
    "\n",
    "        best_score = -1\n",
    "        best_idx = None\n",
    "        for i in range(len(curr)):\n",
    "            trial = curr[:i] + curr[i+1:]\n",
    "            if len(trial) < min_size:\n",
    "                continue\n",
    "            R_trial = R[np.ix_(trial, trial)]\n",
    "            try:\n",
    "                eigvals = np.linalg.eigvalsh(R_trial)\n",
    "                pve_candidate = eigvals[-1] / len(trial)\n",
    "                if pve_candidate > best_score:\n",
    "                    best_score = pve_candidate\n",
    "                    best_idx = i\n",
    "            except:\n",
    "                continue\n",
    "        if best_idx is not None:\n",
    "            curr.pop(best_idx)\n",
    "        else:\n",
    "            return None  # 无法修复\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47836090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_block_with_pca1_info(\n",
    "    block: dict,\n",
    "    R_full: np.ndarray,\n",
    "    z_gwas_full: np.ndarray,\n",
    "    z_qtl_full: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    为已修剪的 block 添加可用于后续前向选择的统计量。\n",
    "    关键转换：将 evaluate_and_prune_block 输出的 L2-unitized loadings\n",
    "              转换为满足 Var(PC1) = 1 的建模尺度载荷。\n",
    "    Args:\n",
    "        block: 来自 evaluate_and_prune_block 的输出字典，包含:\n",
    "               - 'snps': np.array of int, SNP 索引\n",
    "               - 'snp_ids': list of str, SNP ID 列表\n",
    "               - 'loadings': np.array, PC1 载荷（L2 归一化，即 ||w||=1）\n",
    "               - 'pve': float, PC1 解释方差比例\n",
    "        R_full: (p, p) float, 全局 LD 矩阵（对称、标准化）\n",
    "        z_gwas_full: (p,) float, GWAS marginal Z 分数\n",
    "        z_qtl_full: (p,) float, QTL marginal Z 分数（可选用途）\n",
    "    Returns:\n",
    "        enhanced_block: dict, 原始 block 的增强版本，新增字段：\n",
    "            - 'loading_weights': np.array, 满足 w^T R_block w = 1 的载荷\n",
    "            - 'z_gwas_block': float, block 代表 Z（基于标准化 PC1）\n",
    "            - 'z_qtl_block': float, QTL 方向上的 block Z\n",
    "            - 'r_to_others': np.array (p,), block 与所有 SNP 的加权相关性\n",
    "    \"\"\"\n",
    "    snps_in_block = block['snps']\n",
    "    if len(snps_in_block) == 0:\n",
    "        raise ValueError(\"Block has no SNPs.\")\n",
    "    if R_full.shape[0] != R_full.shape[1]:\n",
    "        raise ValueError(\"R_full must be square.\")\n",
    "    p = R_full.shape[0]\n",
    "    if len(z_gwas_full) != p or len(z_qtl_full) != p:\n",
    "        raise ValueError(\"z_gwas_full and z_qtl_full must have length p.\")\n",
    "    # --- 提取 block 内部信息 ---\n",
    "    members_idx = np.array(snps_in_block)\n",
    "    R_block = R_full[np.ix_(members_idx, members_idx)]  # (m, m)\n",
    "    w_l2 = np.array(block['loadings'])  # L2-unitized from pruning step\n",
    "\n",
    "    if len(w_l2) != len(members_idx):\n",
    "        raise ValueError(\"Length of 'loadings' does not match number of SNPs in block.\")\n",
    "    \n",
    "    # --- 关键：归一化使得 Var(PC1) = 1，即：w^T R_block w = 1 ---\n",
    "    var_pc1 = w_l2 @ R_block @ w_l2\n",
    "    if var_pc1 < 1e-10:\n",
    "        raise ValueError(\n",
    "            f\"PC1 variance ({var_pc1:.2e}) too small. \"\n",
    "            \"Likely due to near-collinear SNPs or numerical instability.\"\n",
    "        )\n",
    "    scaling_factor = np.sqrt(var_pc1)\n",
    "    w_model = w_l2 / scaling_factor  # now w_model^T R_block w_model = 1\n",
    "    \n",
    "    var_pc1_normalized = w_model @ R_block @ w_model\n",
    "    assert np.isclose(var_pc1_normalized, 1.0, atol=1e-5), \\\n",
    "        f\"Normalization failed: Var(PC1) = {var_pc1_normalized:.3f} ≠ 1.0\" ## \n",
    "\n",
    "    # --- 校正方向：确保与 GWAS 信号同向 ---\n",
    "    z_sub = z_gwas_full[members_idx]\n",
    "    if w_model @ z_sub < 0:\n",
    "        w_model = -w_model\n",
    "    # --- 计算 block-level Z 分数 ---\n",
    "    z_gwas_block = float(w_model @ z_sub)\n",
    "    z_qtl_block = float(w_model @ z_qtl_full[members_idx])\n",
    "    # --- 计算 block 与所有 SNP 的加权 LD（相关性尺度）---\n",
    "    # r_block,j = Σ_k w_k * r_k,j\n",
    "    r_to_others = w_model @ R_full[members_idx, :]  # (p,)\n",
    "    # --- 构建增强 block ---\n",
    "    enhanced_block = block.copy()\n",
    "    enhanced_block.update({\n",
    "        'loading_weights': w_model,           # 用于建模的标准化载荷\n",
    "        'z_gwas_block': z_gwas_block,         # 标准化后的 block Z\n",
    "        'z_qtl_block': z_qtl_block,\n",
    "        'r_to_others': r_to_others            # 长度为 p，包含所有 SNP的相互R\n",
    "    })\n",
    "\n",
    "    return enhanced_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bed740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enriched_blocks_pipeline(\n",
    "    R: np.ndarray,\n",
    "    z_gwas: np.ndarray,\n",
    "    z_qtl: np.ndarray,\n",
    "    snp_ids: np.ndarray\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    从无到有构建 block，完成：构建 → 去重 → 修剪 → 信息增强\n",
    "    输出：\n",
    "        - enhanced blocks\n",
    "        - 剩余 SNP 索引\n",
    "        - block-block 相关性矩阵\n",
    "        - 扩展的 LD 矩阵 R_extended (p+B, p+B)，支持 SNP + block 统一建模\n",
    "        - 映射表：block 在扩展矩阵中的位置\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'blocks': list of enhanced_block,\n",
    "            'remaining_snp_idx': np.array,\n",
    "            'block_block_r_matrix': (B, B),\n",
    "            'R_extended': (p+B, p+B),\n",
    "            'block_positions_in_extended': list of int,  # 长度 B，表示每个 block 在 R_extended 中的列索引\n",
    "        }\n",
    "    \"\"\"\n",
    "    p = R.shape[0]\n",
    "\n",
    "    assert len(z_gwas) == p and len(z_qtl) == p and len(snp_ids) == p, \"输入维度不匹配\"\n",
    "    if p == 0:\n",
    "        return {\n",
    "            'blocks': [],\n",
    "            'remaining_snp_idx': np.array([]),\n",
    "            'block_block_r_matrix': np.array([]).reshape(0, 0),\n",
    "            'R_extended': np.zeros((0, 0)),\n",
    "            'block_positions_in_extended': []\n",
    "        }\n",
    "\n",
    "    # Step 1: 构建 raw blocks\n",
    "    candidate_blocks = find_maximal_clique_blocks(R, snp_ids, r_min=0.8)\n",
    "    if not candidate_blocks:\n",
    "        return {\n",
    "            'blocks': [],\n",
    "            'remaining_snp_idx': np.arange(p),\n",
    "            'block_block_r_matrix': np.array([]).reshape(0, 0),\n",
    "            'R_extended': R.copy(),  # 原始 R\n",
    "            'block_positions_in_extended': []\n",
    "        }\n",
    "\n",
    "    # Step 2: 去重函数（严格无重叠）\n",
    "    valid_candidates = resolve_block_overlap(candidate_blocks, R)  \n",
    "\n",
    "    # Step 3: 修剪（基于 PVE ≥ 0.7）\n",
    "    pruned_blocks = []\n",
    "    for blk in valid_candidates:\n",
    "        pruned = evaluate_and_prune_block(\n",
    "            block=blk,\n",
    "            R=R,\n",
    "            pve_min=0.7,\n",
    "            min_size=2\n",
    "        )\n",
    "        if pruned is not None:\n",
    "            pruned_blocks.append(pruned)\n",
    "\n",
    "    if not pruned_blocks:\n",
    "        return {\n",
    "            'blocks': [],\n",
    "            'remaining_snp_idx': np.arange(p),\n",
    "            'block_block_r_matrix': np.array([]).reshape(0, 0),\n",
    "            'R_extended': R.copy(),\n",
    "            'block_positions_in_extended': []\n",
    "        }\n",
    "\n",
    "    # Step 4: 增强信息（添加 z_block, r_to_others 等）\n",
    "    enriched_blocks = [\n",
    "        enrich_block_with_pca1_info(blk, R, z_gwas, z_qtl)\n",
    "        for blk in pruned_blocks\n",
    "    ]\n",
    "\n",
    "    # Step 5: 计算未被 block 覆盖的 SNP\n",
    "    used_snps = set()\n",
    "    for blk in enriched_blocks:\n",
    "        used_snps.update(blk['snps']) ## 为block内部的snp index内容\n",
    "        \n",
    "    remaining_snp_idx = np.array(sorted(set(range(p)) - used_snps)) ## 生成所有未被使用的SNP_index\n",
    "\n",
    "    # === Step 6: 计算 block-block 相关性矩阵 ===\n",
    "    n_blocks = len(enriched_blocks)\n",
    "    block_block_r_matrix = np.eye(n_blocks)\n",
    "    for i in range(n_blocks):\n",
    "        blk_i = enriched_blocks[i]\n",
    "        w_i = blk_i['loading_weights']\n",
    "        idx_i = blk_i['snps']\n",
    "        for j in range(i + 1, n_blocks):\n",
    "            blk_j = enriched_blocks[j]\n",
    "            w_j = blk_j['loading_weights']\n",
    "            idx_j = blk_j['snps']\n",
    "            R_sub = R[np.ix_(idx_i, idx_j)]\n",
    "            r_ij = w_i @ R_sub @ w_j\n",
    "            block_block_r_matrix[i, j] = r_ij\n",
    "            block_block_r_matrix[j, i] = r_ij\n",
    "\n",
    "    # === Step 7: 全部计算完成，开始构建扩展 R 矩阵 R_extended: (p + B) x (p + B) ===\n",
    "    B = n_blocks\n",
    "    R_extended = np.zeros((p + B, p + B))\n",
    "\n",
    "    # 1. 原始 SNP-SNP 相关性\n",
    "    R_extended[:p, :p] = R    ## 原始赋值\n",
    "\n",
    "    # 2. SNP - Block 相关性（利用每个 block 的 r_to_others）\n",
    "    block_positions = []\n",
    "    for b_idx, block in enumerate(enriched_blocks):\n",
    "        pos_in_extended = p + b_idx                # block b_idx 放在第 p + b_idx 列\n",
    "        block_positions.append(pos_in_extended)\n",
    "        r_to_others = block['r_to_others']         # (p,) 向量，block PC1 与所有 SNP 的相关性\n",
    "        \n",
    "        # 填入 SNP - block 行/列\n",
    "        R_extended[:p, pos_in_extended] = r_to_others\n",
    "        R_extended[pos_in_extended, :p] = r_to_others  # 对称\n",
    "\n",
    "    # 3. Block - Block 相关性\n",
    "    R_extended[p:, p:] = block_block_r_matrix\n",
    "\n",
    "    # === 返回结果 ===\n",
    "    return {\n",
    "        'blocks': enriched_blocks,\n",
    "        'remaining_snp_idx': remaining_snp_idx,\n",
    "        'block_block_r_matrix': block_block_r_matrix,\n",
    "        'R_extended': R_extended,\n",
    "        'block_positions_in_extended': block_positions,  # 可溯源：第 i 个 block 在 R_extended 中的位置\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_sigma_ire(z_cond, tol=1e-2, max_iter=10, return_diagnostics=False):\n",
    "    \"\"\"\n",
    "    使用迭代加权中位数法估计残差 Z 的背景方差 σ²\n",
    "    假设大多数 SNP 属于噪声（z_i ~ N(0, σ²)），而少数为真信号\n",
    "    Args:\n",
    "        z_cond: (p,) conditional Z 向量\n",
    "        tol: 收敛阈值\n",
    "        max_iter: 最大迭代次数\n",
    "        return_diagnostics: 是否返回诊断信息        \n",
    "    Returns:\n",
    "        sigma2: 估计的背景方差，限制在合理范围内 [0.8, 5.0]\n",
    "    \"\"\"\n",
    "    z = np.asarray(z_cond).flatten()\n",
    "    if len(z) == 0:\n",
    "        return (1.0, {}) if return_diagnostics else 1.0\n",
    "    ## 初始化\n",
    "    sigma2_initial =np.median(z**2) / 0.454936448\n",
    "    sigma2 = sigma2_initial\n",
    "    \n",
    "    for iter_idx in range(max_iter):\n",
    "        # 高斯核权重：|z| 越大，权重越小\n",
    "        w = np.exp(-z**2 / (2 * sigma2 + 1e-8))\n",
    "        # 加权中位数计算\n",
    "        z2 = z**2\n",
    "        sorted_idx = np.argsort(z2)\n",
    "        z2_sorted = z2[sorted_idx]\n",
    "        w_sorted = w[sorted_idx]\n",
    "        cumw = np.cumsum(w_sorted)\n",
    "        total_weight = cumw[-1]\n",
    "        target = 0.5 * total_weight\n",
    "                \n",
    "        weighted_median_z2 = z2_sorted[np.searchsorted(cumw, target, side='right') - 1]\n",
    "        sigma2_new = weighted_median_z2 / 0.454936448\n",
    "        \n",
    "        sigma2_new = np.clip(sigma2_new, 0.8, 5.0)\n",
    "        \n",
    "        # 检查收敛\n",
    "        if abs(sigma2_new - sigma2) < tol:\n",
    "            sigma2 = sigma2_new\n",
    "            break\n",
    "        sigma2 = sigma2_new\n",
    "\n",
    "    return sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@limit_warnings()\n",
    "def compute_conditional_z(\n",
    "    z: np.ndarray,\n",
    "    R: np.ndarray,\n",
    "    selected_indices: list,\n",
    "    U_trunc: np.ndarray,      # 全局谱截断特征向量\n",
    "    Lambda_trunc: np.ndarray  # 全局谱截断特征值\n",
    "):\n",
    "    \"\"\"\n",
    "    使用全局谱截断稳定计算 conditional Z (COJO 思想)\n",
    "    \"\"\"\n",
    "    if len(selected_indices) == 0:\n",
    "        return z.copy()\n",
    "        \n",
    "    S = selected_indices\n",
    "    R_full_sub = R[:, S]      # (p, |S|) 从所有 SNP 到已选 SNP 的 LD\n",
    "    z_selected = z[S]         # (|S|,) 已选 SNP 的 Z 分数\n",
    "    R_sub = R[np.ix_(S, S)]   # (|S|, |S|) 已选 SNP 之间的 LD\n",
    "\n",
    "    try:\n",
    "        U_global_S = U_trunc[S, :]  # (|S|, k) 已选 SNP 在主成分空间中的表示\n",
    "        R_sub_inv = U_global_S @ np.diag(1.0 / Lambda_trunc) @ U_global_S.T\n",
    "        beta = R_sub_inv @ z_selected\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"compute_conditional_z 谱截断求逆失败: {e}\")\n",
    "        try:\n",
    "            beta = np.linalg.solve(R_sub, z_selected)\n",
    "        except:\n",
    "            beta = np.linalg.pinv(R_sub) @ z_selected\n",
    "\n",
    "    # COJO 核心：z_cond = z - R[:,S] @ β\n",
    "    proj_mean = R_full_sub @ beta\n",
    "    z_cond = z - proj_mean\n",
    "    return z_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d725d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spectral_truncation(R: np.ndarray, threshold: float = None):\n",
    "    \"\"\"\n",
    "    对 LD 矩阵进行谱截断，返回显著特征值对应的特征向量和特征值\n",
    "    \"\"\"\n",
    "    eigenvals, eigenvecs = np.linalg.eigh(R)    \n",
    "    idx = np.argsort(eigenvals)[::-1]\n",
    "    eigenvals = eigenvals[idx]\n",
    "    eigenvecs = eigenvecs[:, idx]\n",
    "    if threshold is None:\n",
    "        threshold = max(0.2, 1e-6 * eigenvals[0])  # 修正：使用合理的默认值\n",
    "    \n",
    "    keep = eigenvals > threshold\n",
    "    if not np.any(keep):\n",
    "        keep[0] = True  # 至少保留最大的一个\n",
    "    U_trunc = eigenvecs[:, keep]\n",
    "    Lambda_trunc = eigenvals[keep]\n",
    "    print(f\"   apply_spectral_truncation 诊断:\")\n",
    "    print(f\"     - 原始特征值范围: [{eigenvals.min():.6f}, {eigenvals.max():.6f}]\")\n",
    "    print(f\"     - 截断阈值: {threshold:.6f}\")\n",
    "    print(f\"     - 保留特征值数: {len(Lambda_trunc)}/{len(eigenvals)}\")\n",
    "    print(f\"     - Lambda_trunc 范围: [{Lambda_trunc.min():.6f}, {Lambda_trunc.max():.6f}]\")\n",
    "    return U_trunc, Lambda_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcdd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@limit_warnings(max_count=30)\n",
    "def forward_selection(\n",
    "    z_raw: np.ndarray,\n",
    "    R: np.ndarray,\n",
    "    snp_ids: list,\n",
    "    U_trunc: np.ndarray,\n",
    "    sigma2: float,\n",
    "    Lambda_trunc: np.ndarray,\n",
    "    target_completion: float = 0.9):\n",
    "    \"\"\"\n",
    "    基于 conditional Z 和 LD-adjusted 伪 R² 的前向选择算法\n",
    "    \"\"\"\n",
    "    p = len(z_raw)\n",
    "    print(f\"🔍 forward_selection 诊断:\")\n",
    "    print(f\"   - 输入维度: p = {p}\")\n",
    "    print(f\"   - U_trunc 形状: {U_trunc.shape}\")\n",
    "    print(f\"   - Lambda_trunc 形状: {Lambda_trunc.shape}\")\n",
    "    print(f\"   - Lambda_trunc 范围: [{Lambda_trunc.min():.6f}, {Lambda_trunc.max():.6f}]\")\n",
    "    \n",
    "    if snp_ids is None:\n",
    "        snp_ids = [f\"SNP_{i}\" for i in range(p)]\n",
    "\n",
    "    # === 1. 背景方差校正 ===\n",
    "    z = z_raw / np.sqrt(sigma2)\n",
    "    print(f\"   - 背景方差估计: {sigma2:.4f}\")\n",
    "    print(f\"   - 校正后 Z 范围: [{z.min():.4f}, {z.max():.4f}]\")\n",
    "    print(f\"   - 校正后 Z 均值: {z.mean():.6f}\")\n",
    "    \n",
    "    # === 2. 构造全局能量算子 ===\n",
    "    P = U_trunc @ np.diag(1.0 / Lambda_trunc) @ U_trunc.T\n",
    "    print(f\"   - 能量算子 P 形状: {P.shape}\")\n",
    "    \n",
    "    # === 3. 总信号能量 ===\n",
    "    E_total = z @ P @ z\n",
    "    print(f\"   - 总信号能量 E_total: {E_total:.6f}\")\n",
    "\n",
    "    # === 4. 初始化状态 ===\n",
    "    selected_indices = []\n",
    "    remaining_mask = np.ones(p, dtype=bool)\n",
    "    step = 0\n",
    "    completion = 0.0  # 初始化 completion\n",
    "    print(f\"🚀 开始前向选择迭代...\")\n",
    "    # === 5. 主循环 ===\n",
    "    while True:\n",
    "        step += 1\n",
    "        if step > p + 10:  # 防止无限循环\n",
    "            print(f\"⚠️  迭代次数过多 ({step})，强制退出\")\n",
    "            break\n",
    "        # --- 5.1 计算当前 conditional Z ---\n",
    "        try:\n",
    "            z_cond = compute_conditional_z(z, R, selected_indices, U_trunc, Lambda_trunc)\n",
    "            print(f\"   - z_cond 计算成功，范围: [{z_cond.min():.4f}, {z_cond.max():.4f}]\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ z_cond 计算失败: {e}\")\n",
    "            break\n",
    "        try:\n",
    "            E_residual = z_cond @ P @ z_cond\n",
    "            E_explained = E_total - E_residual\n",
    "            completion = E_explained / E_total if E_total > 1e-8 else 1.0\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 能量计算失败: {e}\")\n",
    "            break\n",
    "\n",
    "        if completion >= target_completion:\n",
    "            print(f\"✅ 达到目标完成度 {target_completion}，停止\")\n",
    "            break\n",
    "        if np.max(np.abs(z_cond[remaining_mask])) < 1.645:\n",
    "            print(f\"⏹️  最大条件 |z| < 1.645，停止\")\n",
    "            break\n",
    "        if len(selected_indices) == p:\n",
    "            print(f\"⏹️  所有 SNP 已选择，停止\")\n",
    "            break\n",
    "\n",
    "        print(f\"   - 寻找候选 SNP...\")\n",
    "        best_idx = None\n",
    "        best_completion = completion\n",
    "\n",
    "        candidate_count = 0\n",
    "        valid_candidates = 0\n",
    "        remaining_indices = np.where(remaining_mask)[0]\n",
    "        sorted_remaining = remaining_indices[np.argsort(np.abs(z_cond[remaining_indices]))[::-1]]\n",
    "        for idx in np.where(sorted_remaining)[0][:20]:  \n",
    "            candidate_count += 1\n",
    "            temp_selected = selected_indices + [idx]\n",
    "            try:\n",
    "                z_cond_temp = compute_conditional_z(z, R, temp_selected, U_trunc, Lambda_trunc)\n",
    "                E_residual_temp = z_cond_temp @ P @ z_cond_temp\n",
    "                E_explained_temp = E_total - E_residual_temp\n",
    "                completion_temp = E_explained_temp / E_total if E_total > 1e-8 else 1.0\n",
    "\n",
    "                if completion_temp > best_completion:\n",
    "                    best_completion = completion_temp\n",
    "                    best_idx = idx\n",
    "                    valid_candidates += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     候选 SNP {idx} 失败: {e}\")\n",
    "                continue\n",
    "        print(f\"   - 候选 SNP 检查: {candidate_count} 个, 本次迭代有效: {valid_candidates} 个\")\n",
    "        if best_idx is None:\n",
    "            print(\"⚠️  没有找到能提升 completion 的 SNP\")\n",
    "            break\n",
    "\n",
    "        selected_snp_id = snp_ids[best_idx]\n",
    "        print(f\"➡️  选择 SNP: {selected_snp_id} (index {best_idx})\")\n",
    "        selected_indices.append(best_idx)\n",
    "        remaining_mask[best_idx] = False\n",
    "\n",
    "    print(f\"\\n🏁 前向选择完成:\")\n",
    "    print(f\"   - 总步数: {step}\")\n",
    "    print(f\"   - 最终选中 SNP 数: {len(selected_indices)}\")\n",
    "    print(f\"   - 最终 completion: {completion:.6f}\")\n",
    "    \n",
    "    # === 6. 返回结果 ===\n",
    "    return {\n",
    "        'selected_indices': selected_indices,\n",
    "        'selected_snp_ids': [snp_ids[i] for i in selected_indices],\n",
    "        'n_selected': len(selected_indices),\n",
    "        'final_completion': float(completion)  # 新增输出项\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_selection_paths(\n",
    "    blocks: list,           # list of dict, 每个 block 包含 z_gwas_block, z_qtl_block, snps, 等等\n",
    "    Z: np.ndarray,          # p, 为原始 Z 向量\n",
    "    R_extended: np.ndarray, # (p + B, p + B)，已构建好的扩展 R\n",
    "    snp_list: list,         # (p,)，SNP ID 列表\n",
    "    analysis_type: str,     # \"gwas\" or \"qtl\"\n",
    "    remaining_snp_idx: np.ndarray,  # 自由池 SNP 索引\n",
    "    n_bootstraps: int = 100,\n",
    "    z_perturb_sd: float = 0.01\n",
    "):\n",
    "    '''\n",
    "    基于 bootstrap 的稳定 SNP/block 选择分析\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict 包含以下字段：\n",
    "        - 'R_clean': 用于分析的 LD 矩阵\n",
    "        - 'Z_clean': 用于分析的 Z 分数向量\n",
    "        - 'Z_extend': 扩展的 Z 向量（包含 block 信息）\n",
    "        - 'sigma2': 背景方差估计\n",
    "        - 'all_selected_paths': 每次 bootstrap 的选择路径\n",
    "        - 'selection_counter': 选择计数器\n",
    "        - 'selection_frequency': 选择频率\n",
    "        - 'stable_snp_id': 频率 > 90% 的稳定 SNP/block ID\n",
    "        - 'snp_list_clean': 清理后的 SNP ID 列表\n",
    "        - 'clean_indices': 清理后的索引\n",
    "        - 'n_free_snps': 自由 SNP 数量\n",
    "        - 'n_blocks': block 数量\n",
    "        - 'avg_completion': 平均信号完成度  # 新增\n",
    "    '''\n",
    "    \n",
    "    # === Step 1: 基本验证 ===\n",
    "    p = len(Z)\n",
    "    B = len(blocks)\n",
    "    assert len(snp_list) == p\n",
    "    assert R_extended.shape == (p + B, p + B)\n",
    "    # === Step 2: 构建扩展 Z 向量 ===\n",
    "    Z_extended = np.zeros(p + B)\n",
    "    Z_extended[:p] = Z\n",
    "    SNPlist_extended = list(snp_list)\n",
    "\n",
    "    # 添加 block 信息\n",
    "    for i, block in enumerate(blocks):\n",
    "        pos = p + i\n",
    "        if analysis_type.lower() == 'gwas':\n",
    "            z_block = block['z_gwas_block']\n",
    "        elif analysis_type.lower() == 'qtl':\n",
    "            z_block = block['z_qtl_block']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown analysis_type: {analysis_type}\")\n",
    "        Z_extended[pos] = z_block\n",
    "        lead_snp = block['snp_ids'][0]\n",
    "        block_id = f\"block|{lead_snp}\"\n",
    "        SNPlist_extended.append(block_id)\n",
    "    # === Step 3: 构建 clean 空间 ===\n",
    "    M = len(remaining_snp_idx)  # 自由 SNP 数量\n",
    "    N_clean = M + B\n",
    "    clean_indices = []\n",
    "    clean_indices.extend(remaining_snp_idx.tolist())\n",
    "    block_extended_positions = [p + i for i in range(B)]\n",
    "    clean_indices.extend(block_extended_positions)\n",
    "    assert len(clean_indices) == N_clean\n",
    "\n",
    "    # 提取 clean R 和 Z\n",
    "    R_clean = R_extended[np.ix_(clean_indices, clean_indices)]\n",
    "    Z_clean = Z_extended[clean_indices]\n",
    "    snp_list_clean = [snp_list[idx] for idx in remaining_snp_idx] + [f\"block|{block['snp_ids'][0]}\" for block in blocks]\n",
    "\n",
    "    # === Step 4: 预处理步骤 ===\n",
    "    estimate_sigma = estimate_sigma_ire(Z_clean)             # 在 clean 空间中直接估计\n",
    "    U_trunc, Lambda_trunc = apply_spectral_truncation(R_clean)\n",
    "    \n",
    "    # === Step 5: Bootstrap 分析 ===\n",
    "    print(f\"🚀 开始 Bootstrap 分析:\")\n",
    "    print(f\"   - Clean 空间大小: {len(clean_indices)} (自由SNP: {M}, Blocks: {B})\")\n",
    "    print(f\"   - 背景方差估计: {estimate_sigma:.4f}\")\n",
    "    print(f\"   - Bootstrap 次数: {n_bootstraps}\")\n",
    "    all_selected = []\n",
    "    selection_counter = Counter()\n",
    "    successful_bootstraps = 0\n",
    "    completion_rates = []  # 新增：记录每次的完成度\n",
    "\n",
    "    for boot_idx in range(n_bootstraps):\n",
    "        if boot_idx % 50 == 0:\n",
    "            print(f\"   - Bootstrap 进度: {boot_idx}/{n_bootstraps}\")\n",
    "        # Z 分数扰动，传入的是纯净的Z-clean\n",
    "        Z_clean_perturbed = Z_clean + np.random.normal(0, z_perturb_sd, size=Z_clean.shape)\n",
    "        try:\n",
    "            result = forward_selection(\n",
    "                z_raw=Z_clean_perturbed,\n",
    "                R=R_clean,\n",
    "                snp_ids=snp_list_clean,\n",
    "                U_trunc=U_trunc,\n",
    "                sigma2= estimate_sigma , \n",
    "                Lambda_trunc=Lambda_trunc,\n",
    "                target_completion=0.9\n",
    "            )\n",
    "            selected_ids = result['selected_snp_ids']\n",
    "            if selected_ids:\n",
    "                successful_bootstraps += 1\n",
    "            all_selected.append(selected_ids)\n",
    "            selection_counter.update(selected_ids)\n",
    "            completion_rates.append(result['final_completion'])  # 新增：记录完成度\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Bootstrap {boot_idx} failed: {e}\")\n",
    "            completion_rates.append(0.0)  # 失败的情况记录为0\n",
    "            continue\n",
    "\n",
    "    print(f\"   - 成功的 Bootstrap 次数: {successful_bootstraps}/{n_bootstraps}\")\n",
    "    \n",
    "    # 计算平均完成度\n",
    "    avg_completion = np.mean(completion_rates) if completion_rates else 0.0\n",
    "    print(f\"   - 平均信号完成度: {avg_completion:.4f}\")\n",
    "    \n",
    "    selection_freq = {\n",
    "        snp_id: count / n_bootstraps\n",
    "        for snp_id, count in selection_counter.items()\n",
    "    }\n",
    "\n",
    "    for snp_id in snp_list_clean:\n",
    "        if snp_id not in selection_freq:\n",
    "            selection_freq[snp_id] = 0.0\n",
    "\n",
    "    stable_snp_id = [snp_id for snp_id, freq in selection_freq.items() if freq > 0.8]\n",
    "    return {\n",
    "        'R_clean': R_clean,\n",
    "        'Z_clean': Z_clean,\n",
    "        'Z_extend': Z_extended,\n",
    "        'sigma2': estimate_sigma,\n",
    "        'U_trunc': U_trunc,              \n",
    "        'Lambda_trunc': Lambda_trunc,       \n",
    "        'all_selected_paths': all_selected,\n",
    "        'selection_counter': selection_counter,\n",
    "        'selection_frequency': selection_freq,\n",
    "        'stable_snp_id': stable_snp_id,\n",
    "        'snp_list_clean': snp_list_clean,\n",
    "        'clean_indices': clean_indices,\n",
    "        'n_free_snps': M,\n",
    "        'n_blocks': B,\n",
    "        'avg_completion': float(avg_completion)  # 新增返回值\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa42144",
   "metadata": {},
   "outputs": [],
   "source": [
    "@limit_warnings()\n",
    "def compute_stable_square_beta(\n",
    "    result: dict,\n",
    "    frequency_threshold: float = 0.9,\n",
    "    min_beta_weight: float = 1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    基于 bootstrap 结果，对高频入选变量进行多变量效应估计\n",
    "    在统一的谱截断空间中进行回归，保证与信号完成度计算一致性\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    result : dict\n",
    "        bootstrap_selection_paths 的返回结果，必须包含：\n",
    "        - 'selection_frequency'\n",
    "        - 'snp_list_clean' \n",
    "        - 'R_clean'\n",
    "        - 'Z_clean'\n",
    "        - 'U_trunc', 'Lambda_trunc' (全局谱截断结果)\n",
    "    frequency_threshold : float, default=0.9\n",
    "        入选频率阈值\n",
    "    min_beta_weight : float\n",
    "        防止 beta² 和为 0 的下界\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        增强的结果字典\n",
    "    \"\"\"\n",
    "    # === Step 1: 提取必要字段 ===\n",
    "    selection_frequency = result['selection_frequency']\n",
    "    snp_list_clean = result['snp_list_clean']\n",
    "    R_clean = result['R_clean']\n",
    "    Z_clean_base = result['Z_clean']\n",
    "    U_trunc_global = result['U_trunc']            # 全局谱截断特征向量\n",
    "    Lambda_trunc_global = result['Lambda_trunc']  # 全局谱截断特征值\n",
    "    \n",
    "    print(f\"📈 compute_stable_square_beta 输入诊断:\")\n",
    "    print(f\"   - selection_frequency 中的 SNP 数: {len(selection_frequency)}\")\n",
    "    print(f\"   - snp_list_clean 长度: {len(snp_list_clean)}\")\n",
    "    print(f\"   - R_clean 形状: {R_clean.shape}\")\n",
    "    print(f\"   - 全局主成分数量: {len(Lambda_trunc_global)}\")\n",
    "\n",
    "    # === Step 2: 找出频率 > threshold 的稳定变量 ===\n",
    "    stable_snp_ids = [\n",
    "        snp_id for snp_id, freq in selection_frequency.items() \n",
    "        if freq >= frequency_threshold\n",
    "    ]\n",
    "\n",
    "    if len(stable_snp_ids) == 0:\n",
    "        print(f\"⚠️ No variable selected with frequency ≥ {frequency_threshold}. Skipping pseudobeta.\")\n",
    "        result.update({\n",
    "            'stable_snps': [],\n",
    "            'stable_snp_indices': [],\n",
    "            'beta_square': {},\n",
    "            'beta_multivar': {},\n",
    "            'beta_sorted': [],\n",
    "            'R_sub_condition_number': 0.0,\n",
    "        })\n",
    "        return result\n",
    "    \n",
    "    # === Step 3: 映射 snp_id → index in clean space ===\n",
    "    snp_to_idx = {snp: idx for idx, snp in enumerate(snp_list_clean)}\n",
    "    stable_indices = [snp_to_idx[snp_id] for snp_id in stable_snp_ids]\n",
    "    # === Step 4: 提取子矩阵 ===\n",
    "    R_sub = R_clean[np.ix_(stable_indices, stable_indices)]\n",
    "    z_sub = Z_clean_base[stable_indices]\n",
    "    k = len(stable_indices)\n",
    "    # === 在统一的谱截断空间中求解 ===\n",
    "    # 提取全局特征向量在子空间中的部分\n",
    "    U_sub = U_trunc_global[stable_indices, :]  # shape: (k, n_components)\n",
    "    # 构造投影后的广义逆：R⁻¹ ≈ U_sub @ diag(1/Λ) @ U_sub.T\n",
    "    Lambda_inv = 1.0 / Lambda_trunc_global\n",
    "    R_sub_inv = U_sub @ np.diag(Lambda_inv) @ U_sub.T\n",
    "    # 求解 beta = R⁻¹ z\n",
    "    beta = R_sub_inv @ z_sub\n",
    "    # 条件数估计（用于诊断）\n",
    "    cond_num = np.linalg.cond(R_sub) if k > 1 else 1.0\n",
    "    \n",
    "    # === Step 6: 计算 beta 平方权重 ===\n",
    "    beta = np.asarray(beta).flatten()\n",
    "    weights = beta ** 2\n",
    "    total_weight = weights.sum()\n",
    "    # 数值稳定性处理\n",
    "    if total_weight < min_beta_weight or total_weight == 0:\n",
    "        beta_squared_weight = np.ones_like(weights) / len(weights) if len(weights) > 0 else np.array([])\n",
    "        compute_stable_square_beta.warn(\n",
    "            f\"Sum of beta^2 too small ({total_weight:.2e}). Using uniform weights.\"\n",
    "        )\n",
    "    else:\n",
    "        beta_squared_weight = weights / total_weight\n",
    "\n",
    "    # === Step 7: 绑定回 snp_id ===\n",
    "    beta_squared_dict = {\n",
    "        snp_id: float(beta2weight) \n",
    "        for snp_id, beta2weight in zip(stable_snp_ids, beta_squared_weight)\n",
    "    }\n",
    "    beta_dict = {\n",
    "        snp_id: float(b) \n",
    "        for snp_id, b in zip(stable_snp_ids, beta)\n",
    "    }\n",
    "    beta_sorted = sorted(beta_squared_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # === Step 8: 更新 result 并返回 ===\n",
    "    result.update({\n",
    "        'stable_snps': stable_snp_ids,\n",
    "        'stable_snp_indices': stable_indices,\n",
    "        'beta_square': beta_squared_dict,\n",
    "        'beta_multivar': beta_dict,\n",
    "        'beta_sorted': beta_sorted,\n",
    "        'R_sub_condition_number': float(cond_num),\n",
    "    })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_causal_discovery(\n",
    "    result: dict, \n",
    "    title: str = \"Causal Discovery by SNP Importance\", \n",
    "    figsize: tuple = (12, 6),\n",
    "    show_legend: bool = True,\n",
    "    bar_alpha: float = 0.8,\n",
    "    bar_colors: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    可视化稳定SNP的因果信号解释能力（仅显示beta平方权重）\n",
    "    可视化内容：\n",
    "    - 横轴：stable SNP 按 beta_square 降序排列\n",
    "    - 纵轴：beta_square（多变量效应平方权重）\n",
    "    - 颜色：block 用红色，普通 SNP 用天蓝色    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    result : dict\n",
    "        compute_stable_square_beta 的输出，必须包含：\n",
    "        - 'beta_square': dict, snp_id -> beta_square 权重\n",
    "        - 'stable_snps': list of str, 稳定 SNP ID 列表\n",
    "        \n",
    "    title : str, default=\"Causal Discovery by SNP Importance\"\n",
    "        图表标题\n",
    "        \n",
    "    figsize : tuple, default=(12, 6)\n",
    "        图像尺寸 (width, height)\n",
    "        \n",
    "    show_legend : bool, default=True\n",
    "        是否显示图例\n",
    "        \n",
    "    bar_alpha : float, default=0.8\n",
    "        柱状图透明度\n",
    "        \n",
    "    bar_colors : dict, optional\n",
    "        自定义颜色映射，格式：{'block': color, 'snp': color}\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (fig, ax) matplotlib 图形对象\n",
    "    \"\"\"\n",
    "    # -----------------------------\n",
    "    # 提取数据\n",
    "    # -----------------------------\n",
    "    beta_square_dict = result.get('beta_square', {})\n",
    "    stable_snp_ids = result.get('stable_snps', [])\n",
    "\n",
    "    # 检查必要数据\n",
    "    if not stable_snp_ids or not beta_square_dict:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.text(0.5, 0.5, 'No stable SNPs/blocks identified', \n",
    "                transform=ax.transAxes, fontsize=14, color='gray', alpha=0.7, \n",
    "                ha='center', va='center')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        ax.set_ylabel(\"\")\n",
    "        plt.title(title, pad=20)\n",
    "        fig.tight_layout()\n",
    "        return fig, ax\n",
    "\n",
    "    # ----------------------------- \n",
    "    # 核心排序：按 beta_square 降序\n",
    "    # -----------------------------\n",
    "    sorted_by_beta2 = sorted(beta_square_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    snp_labels = [item[0] for item in sorted_by_beta2]\n",
    "    beta2_values = [item[1] for item in sorted_by_beta2]\n",
    "    x_pos = np.arange(len(snp_labels))\n",
    "    \n",
    "    # 颜色设置\n",
    "    if bar_colors is None:\n",
    "        bar_colors = {'block': 'red', 'snp': 'skyblue'}\n",
    "    \n",
    "    colors = [\n",
    "        bar_colors['block'] if snp.startswith('block|') else bar_colors['snp'] \n",
    "        for snp in snp_labels\n",
    "    ]\n",
    "\n",
    "    # -----------------------------\n",
    "    # 绘图\n",
    "    # -----------------------------\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=300)\n",
    "    \n",
    "    # 绘制 beta_square 柱状图\n",
    "    bars = ax.bar(x_pos, beta2_values, color=colors, alpha=bar_alpha, width=0.6)\n",
    "    ax.set_xlabel(\"Stable SNPs (ordered by $\\\\beta^2$ weight)\")\n",
    "    ax.set_ylabel(\"$\\\\beta^2$ Weight\", color='black')\n",
    "    ax.tick_params(axis='y', labelcolor='black')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(snp_labels, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_xlim(-0.6, len(snp_labels) - 0.4)\n",
    "\n",
    "    # 图例\n",
    "    if show_legend:\n",
    "        legend_elements = [\n",
    "            Patch(facecolor=bar_colors['snp'], label='SNP'),\n",
    "            Patch(facecolor=bar_colors['block'], label='Block')\n",
    "        ]\n",
    "        ax.legend(\n",
    "            handles=legend_elements,\n",
    "            loc='upper right',\n",
    "            frameon=True,\n",
    "            fontsize=9\n",
    "        )\n",
    "    \n",
    "    plt.title(title, pad=20)\n",
    "    fig.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fine_mapping_for_signal(\n",
    "        gene_df, ld_df, beta_col_gwas, se_col_gwas,\n",
    "        beta_col_qtl, se_col_qtl):\n",
    "    \"\"\"\n",
    "    对某一信号运行完整流程，返回 GWAS 和 QTL 的分析结果,以及block构造信息\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (result_raw_gwas, result_stable_gwas, \n",
    "            result_raw_qtl, result_stable_qtl)\n",
    "    \"\"\"\n",
    "    # === 数据准备 ===\n",
    "    snps_list = ld_df.index.intersection(ld_df.columns).intersection(gene_df.index)\n",
    "    snps_list = snps_list.astype(str)\n",
    "    \n",
    "    ld_matrix_raw = ld_df.loc[snps_list, snps_list].values\n",
    "    \n",
    "    # Z 分数计算\n",
    "    beta_gwas = gene_df.loc[snps_list, beta_col_gwas].values\n",
    "    se_gwas = gene_df.loc[snps_list, se_col_gwas].values\n",
    "    z_gwas = beta_gwas / se_gwas \n",
    "\n",
    "    beta_qtl = gene_df.loc[snps_list, beta_col_qtl].values\n",
    "    se_qtl = gene_df.loc[snps_list, se_col_qtl].values\n",
    "    z_qtl = beta_qtl / se_qtl \n",
    "\n",
    "    # 构建 blocks\n",
    "    blocks_result = build_enriched_blocks_pipeline(ld_matrix_raw, z_gwas, z_qtl, snps_list)\n",
    "    r_extended = blocks_result[\"R_extended\"]\n",
    "    blocks = blocks_result['blocks']\n",
    "    remaining_snp = blocks_result['remaining_snp_idx']\n",
    "\n",
    "    print(f\"📊 GWAS 分析诊断:\")\n",
    "    print(f\"   - 总 SNP 数: {len(snps_list)}\")\n",
    "    print(f\"   - Block 数: {len(blocks)}\")\n",
    "    print(f\"   - 剩余自由 SNP: {len(remaining_snp)}\")\n",
    "    \n",
    "    # === GWAS 分析 ===\n",
    "    result_raw_gwas = bootstrap_selection_paths(\n",
    "        blocks, z_gwas, r_extended, snps_list, \"gwas\", remaining_snp, 100, 0.01\n",
    "    )\n",
    "    \n",
    "    # GWAS 诊断\n",
    "    print(f\"   - Bootstrap 选择路径数: {len(result_raw_gwas.get('all_selected_paths', []))}\")\n",
    "    print(f\"   - 稳定 SNP 数: {len(result_raw_gwas.get('stable_snp_id', []))}\")\n",
    "    if result_raw_gwas.get('stable_snp_id'):\n",
    "        print(f\"   - 稳定 SNP ID: {result_raw_gwas['stable_snp_id'][:5]}...\")\n",
    "    \n",
    "    # GWAS 稳定 SNP 分析\n",
    "    result_stable_gwas = compute_stable_square_beta(result_raw_gwas.copy(), frequency_threshold=0.9)\n",
    "    \n",
    "    # === QTL 分析 ===\n",
    "    print(f\"📊 QTL 分析诊断:\")\n",
    "    print(f\"   - 剩余自由 SNP: {len(remaining_snp)}\")\n",
    "    result_raw_qtl = bootstrap_selection_paths(\n",
    "        blocks, z_qtl, r_extended, snps_list, \"qtl\", remaining_snp, 100, 0.01\n",
    "    )\n",
    "    \n",
    "    # QTL 诊断\n",
    "    print(f\"   - Bootstrap 选择路径数: {len(result_raw_qtl.get('all_selected_paths', []))}\")\n",
    "    print(f\"   - 稳定 SNP 数: {len(result_raw_qtl.get('stable_snp_id', []))}\")\n",
    "    if result_raw_qtl.get('stable_snp_id'):\n",
    "        print(f\"   - 稳定 SNP ID: {result_raw_qtl['stable_snp_id'][:5]}...\")\n",
    "    \n",
    "    # QTL 稳定 SNP 分析\n",
    "    result_stable_qtl = compute_stable_square_beta(result_raw_qtl.copy(), frequency_threshold=0.9)\n",
    "    \n",
    "    return (result_raw_gwas, result_stable_gwas,\n",
    "            result_raw_qtl, result_stable_qtl, blocks_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 新增辅助函数 ===\n",
    "def is_subset_np(a_arr, b_arr):\n",
    "    \"\"\"判断两个字符串数组中每个元素是否满足子集关系\"\"\"\n",
    "    result = []\n",
    "    for a, b in zip(a_arr, b_arr):\n",
    "        if pd.isna(a) or pd.isna(b):\n",
    "            result.append(False)\n",
    "            continue\n",
    "        str_a = str(a).upper()\n",
    "        str_b = str(b).upper()\n",
    "        set_a = set(str_a.split(','))\n",
    "        set_b = set(str_b.split(','))\n",
    "        if any(len(allele) > 1 and allele != '-' for allele in set_a | set_b):\n",
    "            result.append(False)\n",
    "            continue\n",
    "        result.append(set_a <= set_b or set_b <= set_a)\n",
    "    return np.array(result)\n",
    "\n",
    "def classify_and_adjust_beta_vectorized(df):\n",
    "    # 根据实际列名修正\n",
    "    required_cols = {'REF_GWAS', 'ALF_GWAS', 'REF_QTL', 'ALT_QTL', 'beta_QTL'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        missing = required_cols - set(df.columns)\n",
    "        raise ValueError(f\"缺少必需列: {missing}\")\n",
    "    # 使用实际的列名\n",
    "    ref_qtl = df['REF_QTL'].values      # 大写\n",
    "    alt_qtl = df['ALT_QTL'].values      # 大写\n",
    "    ref_gwas = df['REF_GWAS'].values    # 大写\n",
    "    alt_gwas = df['ALF_GWAS'].values    # 根据你的列名是 ALF_GWAS\n",
    "    beta_qtl = df['beta_QTL'].values    # 小写保持不变\n",
    "\n",
    "    cond1 = is_subset_np(alt_gwas, alt_qtl) & is_subset_np(ref_gwas, ref_qtl)  # 方向一致\n",
    "    cond2 = is_subset_np(alt_gwas, ref_qtl) & is_subset_np(ref_gwas, alt_qtl)  # 方向相反\n",
    "    valid_mask = cond1 | cond2\n",
    "    invalid_count = (~valid_mask).sum()\n",
    "    print(f\"共 {invalid_count} 行被丢弃（无法归类）\")\n",
    "\n",
    "    adjusted_beta = np.where(cond2, -beta_qtl, beta_qtl)\n",
    "    df.loc[valid_mask, 'beta_QTL'] = adjusted_beta[valid_mask]\n",
    "\n",
    "    return df[valid_mask].copy()\n",
    "\n",
    "def pivot_ld_to_matrix(ld_df):\n",
    "    \"\"\"将三列格式的 LD 数据转换为对称矩阵\"\"\"\n",
    "    ld_df = ld_df.drop_duplicates(subset=['ID_A', 'ID_B'])\n",
    "    all_snps = sorted(set(ld_df['ID_A']) | set(ld_df['ID_B']))\n",
    "    snp_map = {snp: i for i, snp in enumerate(all_snps)}\n",
    "    n = len(all_snps)\n",
    "    matrix = np.eye(n)  # 默认对角线为1\n",
    "\n",
    "    for _, row in ld_df.iterrows():\n",
    "        i = snp_map[row['ID_A']]\n",
    "        j = snp_map[row['ID_B']]\n",
    "        matrix[i, j] = matrix[j, i] = row['R']\n",
    "\n",
    "    return pd.DataFrame(matrix, index=all_snps, columns=all_snps)\n",
    "\n",
    "\n",
    "# === 日志记录函数 ===\n",
    "\n",
    "def init_log_file(log_path):\n",
    "    \"\"\"初始化日志文件\"\"\"\n",
    "    if not log_path.exists():\n",
    "        pd.DataFrame(columns=[\n",
    "            'timestamp', 'csv_file', 'parquet_file', 'gene', 'status', \n",
    "            'common_snp_count', 'message'\n",
    "        ]).to_csv(log_path, index=False)\n",
    "\n",
    "def log_analysis(log_path, csv_file, parquet_file, gene, status, common_snp_count, message=\"\"):\n",
    "    \"\"\"记录分析日志\"\"\"\n",
    "    log_entry = pd.DataFrame([{\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'csv_file': csv_file,\n",
    "        'parquet_file': parquet_file,\n",
    "        'gene': gene,\n",
    "        'status': status,\n",
    "        'common_snp_count': common_snp_count,\n",
    "        'message': message\n",
    "    }])\n",
    "    log_entry.to_csv(log_path, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c059ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(r\"D:\\desk\\study5_COPDxLC_SMR\\结果文件2\")\n",
    "log_file_path = folder_path / \"log_analysis.csv\"\n",
    "\n",
    "# 初始化日志文件\n",
    "init_log_file(log_file_path)\n",
    "\n",
    "# 获取所有CSV和Parquet文件（一对一映射）\n",
    "csv_files = {f.stem: f for f in folder_path.glob(\"*.csv\")}\n",
    "parquet_files = {f.stem.replace('_LD_matrix', ''): f for f in folder_path.glob(\"*_LD_matrix.parquet\")}\n",
    "\n",
    "print(f\"🔍 找到 {len(csv_files)} 个CSV文件，{len(parquet_files)} 个Parquet文件\")\n",
    "\n",
    "# 遍历每个CSV文件\n",
    "for csv_prefix, csv_path in csv_files.items():\n",
    "    print(f\"\\n👉 正在处理: {csv_prefix}\")\n",
    "    \n",
    "    # 查找对应的Parquet文件\n",
    "    pq_path = parquet_files.get(csv_prefix)\n",
    "    pq_prefix = pq_path.stem if pq_path else \"\"\n",
    "\n",
    "    # 读取CSV文件\n",
    "    # 在数据读取后添加去重逻辑，基于P_GWAS值\n",
    "    df_full = pd.read_csv(csv_path)\n",
    "    original_rows = len(df_full)\n",
    "    \n",
    "    # 如果有P_GWAS列，基于P_GWAS去重，保留P值最小的\n",
    "    if 'P_GWAS' in df_full.columns:\n",
    "        # 按SNP分组，选择P_GWAS最小的行\n",
    "        df_full = df_full.loc[df_full.groupby('SNP')['P_GWAS'].idxmin()]\n",
    "        if len(df_full) < original_rows:\n",
    "            print(f\"🧹 基于P_GWAS去重: {original_rows} → {len(df_full)} 行 (保留P值最小)\")\n",
    "    \n",
    "    csv_snp_count = len(df_full['SNP'].unique())\n",
    "\n",
    "    try:\n",
    "        ld_long = pd.read_parquet(pq_path, engine='fastparquet')\n",
    "        pq_snp_set = set(ld_long['ID_A']) | set(ld_long['ID_B'])\n",
    "        pq_snp_count = len(pq_snp_set)\n",
    "        \n",
    "        coverage_ratio = (pq_snp_count / csv_snp_count) * 100 if csv_snp_count > 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取Parquet失败: {e}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", 0, f\"读取Parquet失败: {e}\")\n",
    "        continue\n",
    "    \n",
    "    output_pkl = folder_path / f\"{pq_prefix}.pkl\"\n",
    "    output_gwas_png = folder_path / f\"{pq_prefix}_gwas.png\"\n",
    "    output_qtl_png = folder_path / f\"{pq_prefix}_qtl.png\"\n",
    "    output_blank = folder_path / f\"{pq_prefix}_blank\"\n",
    "    \n",
    "    # 检查_blank文件\n",
    "    if output_blank.exists():\n",
    "        print(f\"⏭️  已知问题（SNP不足），跳过\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"SKIPPED\", 0, \n",
    "                    f\"已知问题：SNP不足 | CSV SNP数: {csv_snp_count}, Parquet SNP数: {pq_snp_count}, 覆盖率: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    # 检查是否已完成\n",
    "    if output_pkl.exists() and output_gwas_png.exists() and output_qtl_png.exists():\n",
    "        print(f\"✅ 已完成，跳过\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"SKIPPED\", 0, \n",
    "                    f\"已完成，跳过 | CSV SNP数: {csv_snp_count}, Parquet SNP数: {pq_snp_count}, 覆盖率: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    # 调整 beta_QTL 方向（对完整数据进行处理）\n",
    "    try:\n",
    "        df_adjusted = classify_and_adjust_beta_vectorized(df_full)\n",
    "        print(f\"📊 数据过滤：原始 {len(df_full)} 行 → 过滤后 {len(df_adjusted)} 行\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 调整 beta 失败: {e}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", 0, \n",
    "                    f\"调整beta失败: {e} | CSV SNP数: {csv_snp_count}, Parquet SNP数: {pq_snp_count}, 覆盖率: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "\n",
    "    # 设置索引\n",
    "    df_adjusted = df_adjusted.set_index('SNP')\n",
    "    ## 这个是调整后的，然后去LD里面找相关内容\n",
    "    try:\n",
    "        ld_df = pivot_ld_to_matrix(ld_long)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"转换Parquet失败: {e}\"\n",
    "        print(f\"❌ {error_msg}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", 0, \n",
    "                    f\"{error_msg} | CSV SNP数: {csv_snp_count}, Parquet SNP数: {pq_snp_count}, 覆盖率: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "\n",
    "    # 筛选共同SNP（使用调整后的数据索引），反正是LD内容是跟着df_adjusted走的，df_adjusted的意思是调整方向后的，所以我们\n",
    "    snps_common = ld_df.index.intersection(ld_df.columns).intersection(df_adjusted.index)\n",
    "    common_snp_count = len(snps_common)\n",
    "    ## 这里的意思是，如果LD里面没找到。。。那么也删去\n",
    "    if common_snp_count < 3:\n",
    "        print(f\"⚠️ 共同SNP数量不足（{common_snp_count} < 3），跳过分析\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"SKIPPED\", common_snp_count, \n",
    "                    f\"共同SNP数量不足 | CSV SNP数: {csv_snp_count}, Parquet SNP数: {pq_snp_count}, 覆盖率: {coverage_ratio:.2f}%\")\n",
    "        output_blank.touch()\n",
    "        continue\n",
    "    \n",
    "    # 提取共同SNP的子集\n",
    "    ld_df = ld_df.loc[snps_common, snps_common]\n",
    "    df_sub = df_adjusted.loc[snps_common]        \n",
    "    # 到这里完全调整完成\n",
    "    \n",
    "    # 开始分析，输入的是清理后的LD以及 df_sub \n",
    "    try:\n",
    "        (result_raw_gwas, result_stable_gwas,\n",
    "         result_raw_qtl, result_stable_qtl, blocks_result) = run_fine_mapping_for_signal(\n",
    "            df_sub, ld_df, 'BETA_GWAS', 'SE_GWAS', 'beta_QTL', 'SE_QTL'\n",
    "        )\n",
    "        print(f\"✅ 分析完成，共同SNP数: {common_snp_count}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"SUCCESS\", common_snp_count, \n",
    "                    f\"分析完成 | CSV SNP数: {csv_snp_count}, Parquet SNP数: {pq_snp_count}, 覆盖率: {coverage_ratio:.2f}%\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"分析失败: {e}\"\n",
    "        print(f\"❌ {error_msg}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", common_snp_count, \n",
    "                    f\"{error_msg} | CSV SNP数: {csv_snp_count}, Parquet SNP数: {pq_snp_count}, 覆盖率: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    # 绘图\n",
    "    try:\n",
    "        fig_gwas, _ = plot_causal_discovery(result_stable_gwas, f\"{csv_prefix}_gwas\")\n",
    "        fig_qtl, _ = plot_causal_discovery(result_stable_qtl, f\"{csv_prefix}_qtl\")\n",
    "        fig_gwas.savefig(output_gwas_png, dpi=300, bbox_inches='tight')\n",
    "        fig_qtl.savefig(output_qtl_png, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig_gwas)\n",
    "        plt.close(fig_qtl)\n",
    "        print(f\"✅ 绘图完成\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"绘图失败: {e}\"\n",
    "        print(f\"❌ {error_msg}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", common_snp_count, \n",
    "                    f\"{error_msg} | CSV SNP数: {csv_snp_count}, Parquet SNP数: {pq_snp_count}, 覆盖率: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "    \n",
    "    # 保存结果\n",
    "    try:\n",
    "        combined = {\n",
    "            \"gwas_bootstrap\": result_raw_gwas,\n",
    "            \"gwas_stable\": result_stable_gwas,\n",
    "            \"qtl_bootstrap\": result_raw_qtl,\n",
    "            \"qtl_stable\": result_stable_qtl,\n",
    "            \"block\": blocks_result\n",
    "        }\n",
    "        with open(output_pkl, 'wb') as f:\n",
    "            pickle.dump(combined, f)\n",
    "        print(f\"✅ 结果已保存至 {output_pkl}\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"保存失败: {e}\"\n",
    "        print(f\"❌ {error_msg}\")\n",
    "        log_analysis(log_file_path, csv_prefix, pq_prefix, \"\", \"ERROR\", common_snp_count, \n",
    "                    f\"{error_msg} | CSV SNP数: {csv_snp_count}, Parquet SNP数: {pq_snp_count}, 覆盖率: {coverage_ratio:.2f}%\")\n",
    "        continue\n",
    "\n",
    "print(\"✅ 全部任务完成\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
